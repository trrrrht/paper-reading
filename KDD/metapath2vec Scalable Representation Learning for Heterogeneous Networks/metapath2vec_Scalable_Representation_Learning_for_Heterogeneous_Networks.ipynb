{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metapath2vec: Scalable Representation Learning for Heterogeneous Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Metapath2vec**"
      ],
      "metadata": {
        "id": "JuSlyBUOHr-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metapath2vec is an algorithm allowing us to perform random walk on heterogeneous network.\n",
        "\n",
        "In real life, many networks are **heterogeneous**. However, past algorithm mainly focus on learning embedding of homogeneous network, so this paper proposes a new idea about how to perform random walk on heterogeneous network. \n",
        "\n",
        "What is heterogeneous network? It's a network containing **different types** of nodes and edges. For example, in academic network, the nodes could be authors, papers, venues, etc. And the edges could be co-author(author-author), publish(author-paper), belong(paper-venue), etc. In such network, simplily performing random walk would lose the information of this heterogeneous part. \n",
        "\n",
        "The author first define some **metapaths** like, APA, APVPA, etc. APA means two authors co-authored a paper and APVPA means two authors published two papers in the same venue separately. In this case, the heterogeneous information of the nodes and edges are maintained. \n",
        "\n",
        "And for each metapath, they perform random walk to follow the pattern of the path to form the walks. And then they use the model similar to LINE to train on these walks."
      ],
      "metadata": {
        "id": "I7m-hK_2HuV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2_JiQQWrE__P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authornum = 100\n",
        "papernum = 5000\n",
        "venuenum = 20\n",
        "authorset = set(range(0, authornum))\n",
        "paperset = set(range(authornum, authornum + papernum))\n",
        "venueset = set(range(authornum + papernum, authornum + papernum + venuenum))\n",
        "author2paper = {}\n",
        "paper2venue = {}\n",
        "edges = {}\n",
        "\n",
        "tmp = set()\n",
        "for author in authorset:\n",
        "  n = random.randint(1, int(papernum / authornum * 5))\n",
        "  papers = random.sample(list(paperset), n)\n",
        "  for paper in papers:\n",
        "    author2paper[(author, paper)] = 1\n",
        "    edges[(author, paper)] = 1\n",
        "    edges[(paper, author)] = 1\n",
        "    tmp.add(paper)\n",
        "tmp = paperset - tmp\n",
        "for paper in tmp:\n",
        "  author = random.sample(list(authorset), 1)[0]\n",
        "  author2paper[(author, paper)] = 1\n",
        "  edges[(author, paper)] = 1\n",
        "  edges[(paper, author)] = 1\n",
        "\n",
        "tmp = set()\n",
        "for paper in paperset:\n",
        "  venue = random.sample(list(venueset), 1)[0]\n",
        "  tmp.add(venue)\n",
        "  paper2venue[(paper, venue)] = 1\n",
        "  edges[(paper, venue)] = 1\n",
        "  edges[(venue, paper)] = 1\n",
        "\n",
        "tmp = venueset - tmp\n",
        "for venue in tmp:\n",
        "  paper = random.sample(list(paperset), 1)[0]\n",
        "  paper2venue[(paper, venue)] = 1\n",
        "  edges[(paper, venue)] = 1\n",
        "  edges[(venue, paper)] = 1\n",
        "\n",
        "A = 'a'\n",
        "P = 'p'\n",
        "V = 'v'\n",
        "\n",
        "nodes = {A: list(authorset), P: list(paperset), V: list(venueset)}\n",
        "metapaths = [[A, P, V, P, A], [A, P, A]]\n",
        "\n",
        "print(\"num of authors: {0}, num of papers: {1}, num of venues: {2}\".format(len(nodes[A]), len(nodes[P]), len(nodes[V])))\n",
        "print(\"edge of author-paper: {0}\".format(len(author2paper)))\n",
        "print(\"edge of venue-paper: {0}\".format(len(paper2venue)))"
      ],
      "metadata": {
        "id": "nKENOpvtE6Ud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efae94d4-999a-4c0a-e492-acc0e7ec66c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num of authors: 100, num of papers: 5000, num of venues: 20\n",
            "edge of author-paper: 12244\n",
            "edge of venue-paper: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node2neighbors = defaultdict(lambda: defaultdict(dict))\n",
        "for author, paper in author2paper:\n",
        "  node2neighbors[author][P][paper] = author2paper[(author, paper)]\n",
        "  node2neighbors[paper][A][author] = author2paper[(author, paper)]\n",
        "\n",
        "for paper, venue in paper2venue:\n",
        "  node2neighbors[paper][V][venue] = paper2venue[(paper, venue)]\n",
        "  node2neighbors[venue][P][paper] = paper2venue[(paper, venue)]\n",
        "\n",
        "for author, paper in author2paper:\n",
        "  node2neighbors[author][P][paper] /= len(node2neighbors[author][P])\n",
        "  node2neighbors[paper][A][author] /= len(node2neighbors[paper][A])\n",
        "\n",
        "for paper, venue in paper2venue:\n",
        "  node2neighbors[paper][V][venue] /= len(node2neighbors[paper][V])\n",
        "  node2neighbors[venue][P][paper] /= len(node2neighbors[venue][P])"
      ],
      "metadata": {
        "id": "Yrt5jUpxQ9NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class alias():\n",
        "  def __init__(self, probs):\n",
        "    self.n = len(probs)\n",
        "    self.scaledprobs = {}\n",
        "    self.table = {}\n",
        "    self.aliastable = {}\n",
        "    self.small = []\n",
        "    self.big = []\n",
        "    self.keys = list(probs.keys())\n",
        "\n",
        "    for item in probs:\n",
        "      prob = probs[item]\n",
        "      self.scaledprobs[item] = prob * self.n\n",
        "      if self.scaledprobs[item] > 1:\n",
        "        self.big.append(item)\n",
        "      elif self.scaledprobs[item] < 1:\n",
        "        self.small.append(item)\n",
        "      else:\n",
        "        self.table[item] = 1\n",
        "    \n",
        "    while self.small and self.big:\n",
        "      smallitem = self.small.pop()\n",
        "      bigitem = self.big.pop()\n",
        "      newprob = self.scaledprobs[bigitem] - (1 - self.scaledprobs[smallitem])\n",
        "      self.table[smallitem] = self.scaledprobs[smallitem]\n",
        "      self.aliastable[smallitem] = bigitem\n",
        "      self.scaledprobs[bigitem] = newprob\n",
        "      if self.scaledprobs[bigitem] > 1:\n",
        "        self.big.append(bigitem)\n",
        "      elif self.scaledprobs[bigitem] < 1:\n",
        "        self.small.append(bigitem)\n",
        "      else:\n",
        "        self.table[bigitem] = 1\n",
        "    \n",
        "    while self.small:\n",
        "      smallitem = self.small.pop()\n",
        "      self.table[smallitem] = 1\n",
        "    \n",
        "    while self.big:\n",
        "      bigitem = self.big.pop()\n",
        "      self.table[bigitem] = 1\n",
        "\n",
        "  def sampling_one(self):\n",
        "    sample = random.choice(self.keys)\n",
        "    if self.table[sample] >= random.uniform(0, 1):\n",
        "      return sample\n",
        "    else:\n",
        "      return self.aliastable[sample]\n",
        "  \n",
        "  def sampling_n(self, n):\n",
        "    samples = []\n",
        "    for i in range(n):\n",
        "      samples.append(self.sampling_one())\n",
        "    return samples"
      ],
      "metadata": {
        "id": "z9V5k359Ymwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodeprobs = defaultdict(dict)\n",
        "for author, paper in author2paper:\n",
        "  nodeprobs[author][P] = alias(node2neighbors[author][P])\n",
        "  nodeprobs[paper][A] = alias(node2neighbors[paper][A])\n",
        "\n",
        "for paper, venue in paper2venue:\n",
        "  nodeprobs[paper][V] = alias(node2neighbors[paper][V])\n",
        "  nodeprobs[venue][P] = alias(node2neighbors[venue][P])"
      ],
      "metadata": {
        "id": "nbYsLo1oYylP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "walk_len = 10\n",
        "walk_num = 10\n",
        "walks = []\n",
        "for node in range(authornum):\n",
        "  walk_count = 0\n",
        "  while(walk_count < walk_num):\n",
        "    node_count = 1\n",
        "    walk = [node]\n",
        "    t = random.randint(0, 1)\n",
        "    while(node_count < walk_len):\n",
        "      prev = walk[-1]\n",
        "      if prev in authorset:\n",
        "        cur = nodeprobs[prev][P].sampling_one()\n",
        "      elif prev in paperset:\n",
        "        if t == 0:\n",
        "          if walk[-2] in authorset:\n",
        "            cur = nodeprobs[prev][V].sampling_one()\n",
        "          elif walk[-2] in venueset:\n",
        "            cur = nodeprobs[prev][A].sampling_one()\n",
        "        elif t == 1:\n",
        "          cur = nodeprobs[prev][A].sampling_one()\n",
        "      elif prev in venueset:\n",
        "        cur = nodeprobs[prev][P].sampling_one()\n",
        "      walk.append(cur)\n",
        "      node_count += 1\n",
        "    walks.append(walk)\n",
        "    walk_count += 1"
      ],
      "metadata": {
        "id": "7P94i7N_ZaTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"number of walks: {0}\".format(len(walks)))\n",
        "print(walks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqXmHTAud1Qm",
        "outputId": "e2d98152-f8b6-4f2c-d0b7-6d30880af884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of walks: 1000\n",
            "[0, 2159, 5115, 1350, 87, 3749, 5109, 331, 93, 1042]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordsz = authornum + papernum + venuenum\n",
        "def sampling_negedge(dataset, batchsz):\n",
        "  trainset = []\n",
        "  posdata = random.sample(dataset, batchsz)\n",
        "  for source, target in posdata:\n",
        "    trainset.append([source, target, 1])\n",
        "    count = 0\n",
        "    while(count < batchsz):\n",
        "      if target in authorset:\n",
        "        new = random.choice(list(authorset))\n",
        "      elif target in paperset:\n",
        "        new = random.choice(list(paperset))\n",
        "      elif target in venueset:\n",
        "        new = random.choice(list(venueset))\n",
        "      if (source, new) not in edges:\n",
        "        trainset.append([source, new, -1])\n",
        "        break\n",
        "      count += 1\n",
        "  return trainset\n",
        "\n",
        "def one_hot(node):\n",
        "  vec = [0] * wordsz\n",
        "  vec[node] = 1\n",
        "  return vec\n",
        "\n",
        "def gen_data(walks):\n",
        "  dataset = []\n",
        "  for walk in walks:\n",
        "    for i, node in enumerate(walk):\n",
        "      source = node\n",
        "      for j in range(i - windowsz, i + windowsz + 1):\n",
        "        if j != i and j >= 0 and j < len(walk):\n",
        "          target = walk[j]\n",
        "          dataset.append([source, target])\n",
        "  return dataset\n",
        "\n",
        "def tensor_trainset(trainset):\n",
        "  vi = []\n",
        "  vj = []\n",
        "  labels = []\n",
        "  for item in trainset:\n",
        "    vi.append(one_hot(item[0]))\n",
        "    vj.append(one_hot(item[1]))\n",
        "    labels.append(item[2])\n",
        "  return torch.Tensor(vi), torch.Tensor(vj), torch.tensor(labels)"
      ],
      "metadata": {
        "id": "7WuSItSVeQNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windowsz = 2\n",
        "dataset = gen_data(walks)"
      ],
      "metadata": {
        "id": "br67w-fVfu0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "lr = 0.1\n",
        "batchsz = 64\n",
        "batchnum = len(edges) // batchsz\n",
        "featuresz = 32"
      ],
      "metadata": {
        "id": "WF50nwIAd4Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metapath2vec(nn.Module):\n",
        "  def __init__(self, nodenum, featuresz):\n",
        "    super(Metapath2vec, self).__init__()\n",
        "    self.node_embeddings = nn.Linear(nodenum, featuresz, bias=False)\n",
        "    self.sigmoid = nn.LogSigmoid()\n",
        "    self.node_embeddings.weight.data = self.node_embeddings.weight.data.uniform_(\n",
        "            -.5, .5) / featuresz\n",
        "  def forward(self, vi, vj, labels):\n",
        "    viembeddings = self.node_embeddings(vi)\n",
        "    vjembeddings = self.node_embeddings(vj)\n",
        "    inner_product = torch.sum(viembeddings * vjembeddings, 1)\n",
        "    loss = -torch.sum(self.sigmoid(inner_product * labels))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "_ZurJxiEeAQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Metapath2vec(wordsz, featuresz)\n",
        "optimier = optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "R2NCT46redd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    avgloss = 0\n",
        "    for batch in range(batchnum):\n",
        "      trainset = sampling_negedge(dataset, batchsz)\n",
        "      vi, vj, labels = tensor_trainset(trainset)\n",
        "      model.zero_grad()\n",
        "      loss = model(vi, vj, labels)\n",
        "      loss.backward()\n",
        "      optimier.step()\n",
        "      avgloss += loss\n",
        "    avgloss /= batchnum * batchsz\n",
        "    print(\"epoch: {0}, loss: {1}\".format(epoch, avgloss))"
      ],
      "metadata": {
        "id": "pN3Nqoa6ed2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvXcKoO9efKE",
        "outputId": "736d803c-2888-4bd4-f22f-3b0219c44d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 1.374491572380066\n",
            "epoch: 1, loss: 1.2628422975540161\n",
            "epoch: 2, loss: 1.125109314918518\n",
            "epoch: 3, loss: 1.040333867073059\n",
            "epoch: 4, loss: 0.986588716506958\n",
            "epoch: 5, loss: 0.960662841796875\n",
            "epoch: 6, loss: 0.937824547290802\n",
            "epoch: 7, loss: 0.9209144711494446\n",
            "epoch: 8, loss: 0.9173742532730103\n",
            "epoch: 9, loss: 0.9110962748527527\n",
            "epoch: 10, loss: 0.9011550545692444\n",
            "epoch: 11, loss: 0.9040653705596924\n",
            "epoch: 12, loss: 0.8976414203643799\n",
            "epoch: 13, loss: 0.9029335379600525\n",
            "epoch: 14, loss: 0.8958593606948853\n",
            "epoch: 15, loss: 0.8934189081192017\n",
            "epoch: 16, loss: 0.8911520838737488\n",
            "epoch: 17, loss: 0.8945470452308655\n",
            "epoch: 18, loss: 0.8927327990531921\n",
            "epoch: 19, loss: 0.8952814340591431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = model.node_embeddings.weight.T\n",
        "similaity = F.cosine_similarity(embedding.unsqueeze(1), embedding.unsqueeze(0), dim=2)\n",
        "a, idx = torch.sort(similaity, descending=True)\n",
        "k = 4\n",
        "lists=idx[:,1:k+1]\n",
        "for i in range(100):\n",
        "  print(\"[{0}] is similar to \".format(i), end=\"\")\n",
        "  for j in range(k):\n",
        "    print(\"[{0}]\".format(int(lists[i][j])), end=\" \")\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlbXn4OEepPt",
        "outputId": "736e1d0e-0b07-460d-d791-e13962050b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0] is similar to [2989] [1585] [3020] [5043] \n",
            "[1] is similar to [3524] [339] [1360] [522] \n",
            "[2] is similar to [369] [1938] [3732] [4720] \n",
            "[3] is similar to [1051] [3426] [627] [4741] \n",
            "[4] is similar to [2613] [165] [1476] [4823] \n",
            "[5] is similar to [4490] [303] [4319] [3759] \n",
            "[6] is similar to [2589] [4282] [4074] [3698] \n",
            "[7] is similar to [1402] [2803] [2163] [4030] \n",
            "[8] is similar to [4468] [635] [2506] [371] \n",
            "[9] is similar to [3757] [1139] [5020] [3998] \n",
            "[10] is similar to [2682] [4390] [4812] [3131] \n",
            "[11] is similar to [2013] [4974] [4982] [1223] \n",
            "[12] is similar to [1394] [1930] [4006] [488] \n",
            "[13] is similar to [3246] [2592] [1961] [1880] \n",
            "[14] is similar to [3513] [2540] [1560] [2425] \n",
            "[15] is similar to [891] [2311] [2868] [2941] \n",
            "[16] is similar to [2509] [4108] [2695] [1765] \n",
            "[17] is similar to [2094] [2173] [2674] [3614] \n",
            "[18] is similar to [3550] [3048] [1391] [1911] \n",
            "[19] is similar to [1712] [3746] [4426] [395] \n",
            "[20] is similar to [4813] [3440] [1826] [334] \n",
            "[21] is similar to [3283] [2131] [302] [673] \n",
            "[22] is similar to [4124] [4676] [2784] [2893] \n",
            "[23] is similar to [1537] [1105] [1799] [3056] \n",
            "[24] is similar to [2158] [690] [4414] [4192] \n",
            "[25] is similar to [2011] [1375] [292] [4275] \n",
            "[26] is similar to [4441] [4112] [1112] [2708] \n",
            "[27] is similar to [2567] [2153] [4926] [4360] \n",
            "[28] is similar to [444] [2889] [4470] [3310] \n",
            "[29] is similar to [3679] [427] [3296] [4740] \n",
            "[30] is similar to [3724] [3677] [3655] [3451] \n",
            "[31] is similar to [2685] [1090] [2734] [105] \n",
            "[32] is similar to [1719] [3449] [1690] [4533] \n",
            "[33] is similar to [1933] [3706] [3140] [2930] \n",
            "[34] is similar to [2827] [872] [1396] [3653] \n",
            "[35] is similar to [2010] [677] [4240] [2833] \n",
            "[36] is similar to [2516] [4259] [2261] [1764] \n",
            "[37] is similar to [3658] [3341] [3939] [1598] \n",
            "[38] is similar to [2339] [3907] [4122] [4882] \n",
            "[39] is similar to [1069] [1476] [2431] [643] \n",
            "[40] is similar to [4615] [1313] [1565] [5051] \n",
            "[41] is similar to [3053] [1769] [4331] [1146] \n",
            "[42] is similar to [3097] [2168] [2344] [3727] \n",
            "[43] is similar to [4146] [3787] [4445] [4333] \n",
            "[44] is similar to [420] [3115] [763] [2083] \n",
            "[45] is similar to [631] [5032] [1119] [4465] \n",
            "[46] is similar to [2645] [1546] [152] [518] \n",
            "[47] is similar to [651] [2229] [4174] [4113] \n",
            "[48] is similar to [2088] [1204] [2548] [2290] \n",
            "[49] is similar to [2230] [3366] [2105] [3912] \n",
            "[50] is similar to [4127] [5014] [1483] [4803] \n",
            "[51] is similar to [589] [176] [1468] [2902] \n",
            "[52] is similar to [4623] [262] [3847] [3125] \n",
            "[53] is similar to [4388] [3223] [3292] [3985] \n",
            "[54] is similar to [3055] [3800] [4876] [4851] \n",
            "[55] is similar to [4907] [140] [2617] [1080] \n",
            "[56] is similar to [380] [2152] [1020] [2899] \n",
            "[57] is similar to [4685] [2601] [4737] [2418] \n",
            "[58] is similar to [3406] [2386] [1464] [2616] \n",
            "[59] is similar to [3954] [881] [2615] [629] \n",
            "[60] is similar to [209] [3931] [4208] [2985] \n",
            "[61] is similar to [1507] [471] [1819] [4992] \n",
            "[62] is similar to [942] [1159] [2099] [4247] \n",
            "[63] is similar to [3792] [1700] [4007] [4050] \n",
            "[64] is similar to [2656] [2444] [4096] [2198] \n",
            "[65] is similar to [4607] [1640] [1677] [1340] \n",
            "[66] is similar to [4244] [4267] [597] [4352] \n",
            "[67] is similar to [749] [3791] [4506] [2380] \n",
            "[68] is similar to [4368] [5049] [3547] [2953] \n",
            "[69] is similar to [4781] [1399] [2176] [3446] \n",
            "[70] is similar to [4919] [686] [4438] [4073] \n",
            "[71] is similar to [977] [3502] [1185] [3564] \n",
            "[72] is similar to [3650] [3208] [268] [1567] \n",
            "[73] is similar to [4528] [887] [2984] [2666] \n",
            "[74] is similar to [985] [1458] [2845] [337] \n",
            "[75] is similar to [822] [2484] [3261] [4182] \n",
            "[76] is similar to [352] [4391] [1053] [1240] \n",
            "[77] is similar to [2389] [3071] [4652] [3157] \n",
            "[78] is similar to [2243] [1107] [5009] [5016] \n",
            "[79] is similar to [387] [1590] [4160] [3495] \n",
            "[80] is similar to [4284] [3793] [3904] [4673] \n",
            "[81] is similar to [4464] [4509] [4551] [2677] \n",
            "[82] is similar to [2162] [886] [2773] [3170] \n",
            "[83] is similar to [2404] [1154] [1767] [315] \n",
            "[84] is similar to [2292] [3820] [4837] [1102] \n",
            "[85] is similar to [2993] [1637] [804] [1907] \n",
            "[86] is similar to [1368] [187] [4908] [861] \n",
            "[87] is similar to [1014] [1310] [3511] [1444] \n",
            "[88] is similar to [4856] [3093] [3863] [2058] \n",
            "[89] is similar to [2143] [888] [1027] [3396] \n",
            "[90] is similar to [544] [4328] [4700] [1527] \n",
            "[91] is similar to [2337] [2649] [1585] [1513] \n",
            "[92] is similar to [5070] [3061] [3216] [1518] \n",
            "[93] is similar to [931] [1042] [2944] [4649] \n",
            "[94] is similar to [1845] [3986] [1900] [2912] \n",
            "[95] is similar to [4388] [1945] [3270] [2859] \n",
            "[96] is similar to [2887] [4838] [3515] [604] \n",
            "[97] is similar to [3808] [2598] [2308] [294] \n",
            "[98] is similar to [2480] [2260] [3720] [3107] \n",
            "[99] is similar to [4988] [1355] [4513] [2829] \n"
          ]
        }
      ]
    }
  ]
}