{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Lemane**"
      ],
      "metadata": {
        "id": "CSq3yWvQRqYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This paper is based on STRAP. The main contribution of this paper is the training alpha part. This paper first use a unsurprised learning to learning the alpha, which is used as coefficient of power iteration of PPR. For small graphs, the authors use the whole graph to train the model, whereas for big graphs, they use BFS to generate subgraph to get the alpha. After that, they use PPR on the entire graph with these alphas to generate embedding for each nodes for the downstream tasks.\n",
        "\n",
        "Although the source code has some mistakes, the authors still beats SOTA. "
      ],
      "metadata": {
        "id": "YxnCzhugR2pU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kkMjVt2h4TC",
        "outputId": "1a84da4d-1f2c-47c3-8317-ef9659eecfbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Lemane'...\n",
            "remote: Enumerating objects: 309, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (211/211), done.\u001b[K\n",
            "remote: Total 309 (delta 145), reused 240 (delta 78), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (309/309), 22.21 MiB | 18.85 MiB/s, done.\n",
            "Resolving deltas: 100% (145/145), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Josiah96Zhang/Lemane.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L8EiK-6iMQv"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6RUMzsnlH0w",
        "outputId": "518a5659-f2ce-4790-cc1e-6dbafe98654c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fa245a12050>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHAvCl7FlJhK"
      },
      "outputs": [],
      "source": [
        "nepoch = 10\n",
        "lr = 0.01\n",
        "wdecay1 = 0.01\n",
        "nhop = 15\n",
        "sample = 10\n",
        "sample_sup_label_set = False\n",
        "patience = 10\n",
        "data = 'wiki'\n",
        "dist = 'p'\n",
        "param = 5\n",
        "beta = 1.0\n",
        "gamma = 1.0\n",
        "seed = 1629459462\n",
        "task = 'cl'\n",
        "sample_size = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD6UMjT5lntU"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  if seed == 0:\n",
        "      seed = int(time.time())\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  np.random.RandomState(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  torch.backends.cudnn.enabled = False\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  return seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5_qqjBDluJc"
      },
      "outputs": [],
      "source": [
        "def load_info(dataset):\n",
        "  attr_path = \"Lemane/data/\" + dataset + \"/attr.txt\"\n",
        "  if not attr_path or not os.path.exists(attr_path):\n",
        "    raise Exception(\"graph attr file does not exist!\")\n",
        "  with open(attr_path) as fin:\n",
        "    n = int(fin.readline().split(\"=\")[1])\n",
        "    m = int(fin.readline().split(\"=\")[1])\n",
        "    directed = (fin.readline().strip()==\"directed\")\n",
        "  fin.close()\n",
        "  print(\"graph name: {}\".format(dataset))\n",
        "  return n, m, directed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwySms2KmrKI"
      },
      "outputs": [],
      "source": [
        "def sup_label_set(dataset,n):\n",
        "  label_path = \"Lemane/label/\" + dataset + \".txt\"\n",
        "  sup_label_path = \"Lemane/label/\" + dataset + \"_sup.txt\"\n",
        "  labeled_list = []\n",
        "  if dataset in [\"tweibo\", \"orkut\"]:\n",
        "    if not label_path or not os.path.exists(label_path):\n",
        "      raise Exception(\"node label file does not exist!\")\n",
        "    else:\n",
        "      with open(label_path) as f:\n",
        "        for line in f:\n",
        "          vec = line.strip().split()\n",
        "          i = int(vec[0])\n",
        "          labeled_list.append(i)\n",
        "      f.close()\n",
        "    index_list = random.sample(list(range(len(labeled_list))), int(0.05 * len(labeled_list)))\n",
        "    sup_list = []\n",
        "    for i in index_list:\n",
        "      sup_list.append(labeled_list[i])\n",
        "    sup_list.sort()\n",
        "  else:\n",
        "    sup_list = random.sample(list(range(n)), int(0.05 * n))\n",
        "    sup_list.sort()\n",
        "  fout = open(sup_label_path, \"w\")\n",
        "  for i in sup_list:\n",
        "    fout.write(str(i) + \"\\n\")\n",
        "  fout.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfTPJB9nn0r1"
      },
      "outputs": [],
      "source": [
        "def show_info(G):\n",
        "  print('Num of nodes: %d, num of edges: %d, Avg degree: %f, Directed:%s' % (G.number_of_nodes(), G.number_of_edges(), G.number_of_edges()*2./G.number_of_nodes(), str(nx.is_directed(G))))\n",
        "  return G.number_of_edges()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TH4HB8FIngo3"
      },
      "outputs": [],
      "source": [
        "def load_edge(dataset, n, directed, task):\n",
        "  if task == \"lp\":\n",
        "    edgelist_path = \"Lemane/lp_data/train_graph/\" + dataset + \".txt\"\n",
        "  else:\n",
        "    edgelist_path = \"Lemane/data/\" + dataset + \".txt\"\n",
        "  if not edgelist_path or not os.path.exists(edgelist_path):\n",
        "    raise Exception(\"edgelist file does not exist!\")\n",
        "  t1 = time.time()\n",
        "  with open(edgelist_path, 'r') as f:\n",
        "    if directed:\n",
        "      G = nx.DiGraph()\n",
        "    else:\n",
        "      G = nx.Graph()\n",
        "    _ = f.readline()\n",
        "    for line in f:\n",
        "      edge = line.strip().split()\n",
        "      u, v = int(edge[0]), int(edge[1])\n",
        "      G.add_edge(u, v)\n",
        "    for i in range(n):\n",
        "      if i not in G.nodes():\n",
        "        G.add_edge(i,i)\n",
        "  f.close()\n",
        "  t2 = time.time()\n",
        "  print('%fs taken for loading graph' % (t2 - t1))\n",
        "  m = show_info(G)\n",
        "  adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n",
        "  print('%fs taken for generating adj matrix' % (time.time() - t2))\n",
        "  return G, adj, m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrBQUJf-onVG"
      },
      "outputs": [],
      "source": [
        "def get_sample_neg_laplacian(sample, num_sample):\n",
        "  adj_neg = sp.random(num_sample, num_sample, density=sample/num_sample, data_rvs=np.ones, random_state=1628837069)\n",
        "  deg_neg = sp.diags(np.array(adj_neg.sum(1)).flatten())\n",
        "  Lapneg = (deg_neg - adj_neg).tocoo().astype(np.float64)\n",
        "\n",
        "  indices_neg = torch.from_numpy(np.vstack((Lapneg.row, Lapneg.col)).astype(np.int64))\n",
        "  values_neg = torch.from_numpy(Lapneg.data)\n",
        "  shape_neg = torch.Size(Lapneg.shape)\n",
        "\n",
        "  return torch.sparse.DoubleTensor(indices_neg, values_neg, shape_neg), adj_neg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRzn-qf5pmer"
      },
      "outputs": [],
      "source": [
        "def get_trans_prob_mat(adj, sample_list, sample_size, task):\n",
        "  assert len(sample_list) == sample_size\n",
        "  if sample_size == 0:\n",
        "    sample_adj = adj\n",
        "  else:\n",
        "    sample_adj = adj[sample_list]\n",
        "    sample_adj = sp.csc_matrix(sample_adj)[:,sample_list]\n",
        "    sample_adj = sp.coo_matrix(sample_adj)\n",
        "  row_sum = np.array(sample_adj.sum(1))\n",
        "  row_sum[row_sum < 1] = 1\n",
        "  degree = np.array(sample_adj.sum(1)).flatten()\n",
        "  prob = sp.coo_matrix(sample_adj / row_sum).astype(np.float64)\n",
        "  indices = torch.from_numpy(np.vstack((prob.row, prob.col)).astype(np.int64))\n",
        "  values = torch.from_numpy(prob.data)\n",
        "  shape = torch.Size(prob.shape)\n",
        "  if task == 'lp':\n",
        "    adj = sample_adj\n",
        "    return torch.sparse.DoubleTensor(indices, values, shape), adj, torch.DoubleTensor(degree)\n",
        "  if task == 'cl':\n",
        "    return torch.sparse.DoubleTensor(indices, values, shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7AHRcgzrA6K"
      },
      "outputs": [],
      "source": [
        "def load_label(dataset, n):\n",
        "  sup_label_path = \"Lemane/label/\" + dataset + \"_sup.txt\"\n",
        "  Sup_List = []\n",
        "  if not sup_label_path or not os.path.exists(sup_label_path):\n",
        "    sup_label_set(dataset, n)\n",
        "  with open(sup_label_path) as f:\n",
        "    for line in f:\n",
        "      vec = line.strip().split()\n",
        "      i = int(vec[0])\n",
        "      Sup_List.append(i)\n",
        "  f.close()\n",
        "\n",
        "\n",
        "  label_path = \"Lemane/label/\" + dataset + \".txt\"\n",
        "  Node_Label = [[]for i in range(n)]\n",
        "  if not label_path or not os.path.exists(label_path):\n",
        "    raise Exception(\"node label file does not exist!\")\n",
        "  if dataset in [\"tweibo\", \"orkut\"]:\n",
        "    with open(label_path) as f:\n",
        "      for line in f:\n",
        "        vec = line.strip().split()\n",
        "        i = int(vec[0])\n",
        "        if i in Sup_List:\n",
        "          Node_Label[i] = vec[1:]\n",
        "          for j in range(len(Node_Label[i])):\n",
        "            Node_Label[i][j] = int(Node_Label[i][j]) + 1\n",
        "  else:\n",
        "    with open(label_path) as f:\n",
        "      for line in f:\n",
        "        vec = line.strip().split()\n",
        "        i = int(vec[0])\n",
        "        if i in Sup_List:\n",
        "          Node_Label[i] = vec[1:]\n",
        "          for j in range(len(Node_Label[i])):\n",
        "            Node_Label[i][j] = int(Node_Label[i][j])\n",
        "  \n",
        "  f.close()\n",
        "  return Node_Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up6l9i57sKeY"
      },
      "outputs": [],
      "source": [
        "def get_sample_label_list(Node_label, sample_list,  num_label):\n",
        "  if len(sample_list) == 0:\n",
        "    node_label = Node_label\n",
        "  else:\n",
        "    node_label = [Node_label[i] for i in sample_list]\n",
        "\n",
        "  label_list = [ [] for i in range(num_label+1)]\n",
        "  for i in range(len(node_label)):\n",
        "    for j in node_label[i]:\n",
        "      label_list[j].append(i)\n",
        "  label_list = [k for k in label_list if len(k)>0]\n",
        "  binarizer = MultiLabelBinarizer(sparse_output=False, classes=list(range(1,num_label+1)))\n",
        "  node_label = torch.tensor(binarizer.fit_transform(node_label))\n",
        "  return label_list, node_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4IlFSD8vJRG"
      },
      "outputs": [],
      "source": [
        "def get_label_laplacian(L):\n",
        "  Laplabel = []\n",
        "  for i in range(len(L)):\n",
        "    n = len(L[i])\n",
        "    Laplabel_i = sp.coo_matrix(np.eye(n) * n - 1).tocoo().astype(np.float64)\n",
        "    indices_i = torch.from_numpy(np.vstack((Laplabel_i.row, Laplabel_i.col)).astype(np.int64))\n",
        "    values_i = torch.from_numpy(Laplabel_i.data)\n",
        "    shape_i = torch.Size(Laplabel_i.shape)\n",
        "    Laplabel_i = torch.sparse.DoubleTensor(indices_i, values_i, shape_i)\n",
        "    Laplabel.append(Laplabel_i)\n",
        "  return Laplabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqjI2t6vwiX2"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def simple_randomized_torch_svd(B, k, task):\n",
        "  _, n = B.size()\n",
        "  rand_matrix = torch.rand((n,k), dtype=torch.float64).to(device)   \n",
        "  Q, _ = torch.qr(B @ rand_matrix)                                # qr decomposition\n",
        "  Q.to(device)\n",
        "  smaller_matrix = (Q.transpose(0, 1) @ B).to(device)\n",
        "  U_hat, s, V = torch.svd(smaller_matrix, True)                   # matrix decompostion\n",
        "  U_hat.to(device)\n",
        "  U = (Q @ U_hat)\n",
        "\n",
        "  if task == 'lp':\n",
        "    return U @ (s.pow(0.5).diag()), V @ (s.pow(0.5).diag())     # for link prediction\n",
        "  if task == 'cl':\n",
        "    return torch.cat((U @ (s.pow(0.5).diag()),V @ (s.pow(0.5).diag())), 1) # for node classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ezV6_OAwGMC"
      },
      "outputs": [],
      "source": [
        "class ComputeProximity4SVD(nn.Module):\n",
        "  def __init__(self, ngraph, niter, dist, param, nclass=1):\n",
        "    super(ComputeProximity4SVD, self).__init__()\n",
        "    self.ngraph = ngraph\n",
        "    self.niter = niter\n",
        "    self.nclass = nclass\n",
        "    self.params1 = Parameter(torch.DoubleTensor(self.niter + 1))\n",
        "    self.init_params(dist, param)\n",
        "    self.fcs = nn.ModuleList()\n",
        "    self.fcs.append(nn.Linear(256, nclass).double())\n",
        "    torch.nn.init.xavier_uniform_(self.fcs[0].weight)\n",
        "\n",
        "  # compute the teleport probabilities of the random walks generated from Poisson distribution\n",
        "  def poisson_dist(self, t):\n",
        "    K = 100\n",
        "    poisson = [0]*(K+1)\n",
        "    poissonsum = [0]*(K+2)\n",
        "    stay = torch.DoubleTensor(self.niter+1)\n",
        "    poisson[0] = 1.0/math.exp(t)\n",
        "    poissonsum[0] = 0.\n",
        "    poissonsum[1] =  poissonsum[0] + poisson[0]\n",
        "    for i in range(1, K+1):\n",
        "      poisson[i] = poisson[i-1] * t * 1.0 / i\n",
        "      poissonsum[i+1] = poissonsum[i] + poisson[i]\n",
        "\n",
        "    for i in range(self.niter+1):\n",
        "      stay[i] = poisson[i]/(1.0 - poissonsum[i])\n",
        "    return stay\n",
        "\n",
        "  # teleport probabilities initialization\n",
        "  def init_params(self, dist, param):\n",
        "    if dist == 'p':\n",
        "      self.params1.data = self.poisson_dist(param)\n",
        "    if dist == 'g':\n",
        "      self.params1.data = torch.ones(self.niter + 1) * param\n",
        "\n",
        "  # forward propagation process\n",
        "  def forward(self, prob, identity, threshold, task):\n",
        "    hi = identity\n",
        "    prx_mat = hi * self.params1[0]\n",
        "    for i in range(self.niter):\n",
        "      hi = (prob @ hi) * (1 - self.params1[i])\n",
        "      prx_mat = prx_mat + hi * self.params1[i+1]\n",
        "    prx_mat = prx_mat / threshold\n",
        "    prx_mat[prx_mat < 1] = 1.\n",
        "    prx_mat_log = prx_mat.log()\n",
        "    if task == 'lp':\n",
        "      U, V = simple_randomized_torch_svd(prx_mat_log, 128, task)\n",
        "      return U, V\n",
        "    if task == 'cl':\n",
        "      embds_svd = simple_randomized_torch_svd(prx_mat_log, 128, task)\n",
        "      embds = self.fcs[0](embds_svd)\n",
        "      return embds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrB0Z7ziwuEc"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "  length_flag = True\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(prob, identity, threshold, task)\n",
        "  output_prob = F.log_softmax(output, dim=1)\n",
        "  output = torch.nn.functional.normalize(output)\n",
        "  output_0 = output[Label_list[0]]\n",
        "  neg_loss = torch.trace(torch.transpose(output, 0, 1) @ (Lap_neg @ output))\n",
        "\n",
        "  class_loss_fn1 = torch.trace(torch.transpose(output_0, 0, 1) @ (Lap_label[0] @ output_0))\n",
        "  for i in range(1, len(Label_list)):\n",
        "    output_i = output[Label_list[i]]\n",
        "    class_loss_fn1 = class_loss_fn1 + torch.trace(torch.transpose(output_i, 0, 1) @ (Lap_label[i] @ output_i))\n",
        "  if data == \"tweibo\":\n",
        "    class_loss_fn1 = class_loss_fn1  / (neg_loss * len(Label_list))\n",
        "    class_loss_fn2 = - torch.mul(output_prob, node_Label.to(device)).sum() / len(bfs_sample_node_list)\n",
        "\n",
        "  #derivative of alphas on orkut is too small, or we can select larger learning rate, beta and gamma.\n",
        "  elif data == \"orkut\":\n",
        "    class_loss_fn1 = class_loss_fn1  / neg_loss\n",
        "    class_loss_fn2 = - torch.mul(output_prob, node_Label.to(device)).sum()\n",
        "\n",
        "  else:\n",
        "    class_loss_fn1 = class_loss_fn1  / (neg_loss * len(Label_list))\n",
        "    class_loss_fn2 = - torch.mul(output_prob, node_Label.to(device)).sum() / len(bfs_sample_node_list)\n",
        "\n",
        "  loss_fn = class_loss_fn1 * beta + class_loss_fn2 * gamma\n",
        "  loss_fn.backward()\n",
        "\n",
        "  temp_dist = model.params1.clone().cpu().detach().numpy()\n",
        "  optimizer.step()\n",
        "  params_zero = model.params1.data[model.params1.data>=0]\n",
        "  satlen = len(params_zero[params_zero<=1])\n",
        "  if satlen < (nhop+1):\n",
        "    length_flag = False\n",
        "    print(\"Some teleport probabilities are out of range!\")\n",
        "  return loss_fn.item(), temp_dist, length_flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PKvLGo11knJ"
      },
      "outputs": [],
      "source": [
        "def dfs_sampling(G, max_sample):#学长代码似乎写错了\n",
        "  sample_list = set()\n",
        "  while len(sample_list) < 5000:\n",
        "    seed = random.randint(0,G.number_of_nodes()-1)\n",
        "    #print(\"sample seed node: \" + str(seed))\n",
        "    queue = [seed]\n",
        "    while queue:\n",
        "      temp_node = queue.pop()\n",
        "      if temp_node not in sample_list:\n",
        "        sample_list.add(temp_node)\n",
        "        queue.extend(list(set(G.adj[temp_node]) - sample_list))\n",
        "      if len(sample_list) >= max_sample:\n",
        "          return list(sample_list)\n",
        "\n",
        "def neighbor_sampling(G, max_sample):\n",
        "  sample_list = set()\n",
        "  while len(sample_list) < 5000:\n",
        "    seed = random.randint(0,G.number_of_nodes()-1)\n",
        "    #print(\"sample seed node: \" + str(seed))\n",
        "    queue = [seed]\n",
        "    while queue:\n",
        "      temp_node = queue.pop(0)\n",
        "      if temp_node not in sample_list:\n",
        "        sample_list.add(temp_node)\n",
        "        nextneighs = list(set(G.adj[temp_node]) - sample_list)\n",
        "        random.shuffle(nextneighs)\n",
        "        nextneighs = nextneighs[:int(len(nextneighs) * max_sample / G.number_of_nodes())]\n",
        "        queue.extend(nextneighs)\n",
        "      if len(sample_list) >= max_sample:\n",
        "          return list(sample_list)\n",
        "\n",
        "def bfs_sampling(G, max_sample):\n",
        "  sample_list = set()\n",
        "  while len(sample_list) < 5000:\n",
        "    seed = random.randint(0,G.number_of_nodes()-1)\n",
        "    #print(\"sample seed node: \" + str(seed))\n",
        "    queue = [seed]\n",
        "    while queue:\n",
        "      temp_node = queue.pop(0)\n",
        "      if temp_node not in sample_list:\n",
        "        sample_list.add(temp_node)\n",
        "        queue.extend(list(set(G.adj[temp_node]) - sample_list))\n",
        "      if len(sample_list) >= max_sample:\n",
        "          return list(sample_list)\n",
        "\n",
        "def node2vec_sampling(G, max_sample):\n",
        "  gamma = 0.5\n",
        "\n",
        "  sample_list = set()\n",
        "  pre = random.randint(0,G.number_of_nodes()-1)\n",
        "  samples = [pre]\n",
        "  while len(sample_list) < max_sample:\n",
        "    \n",
        "    curs = list(set(G.adj[pre]) - sample_list)\n",
        "    if not curs:\n",
        "      pre = random.choice(samples)\n",
        "      continue\n",
        "\n",
        "    cur = random.choice(curs)\n",
        "    samples.append(cur)\n",
        "    sample_list |= set(samples)\n",
        "    if random.random() < gamma:\n",
        "      pre = cur\n",
        "\n",
        "  return list(sample_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "UBw7V908lts8",
        "outputId": "74739c20-12bf-46f2-f935-02daf9d30f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "graph name: wiki\n",
            "1.093647s taken for loading graph\n",
            "Num of nodes: 4777, num of edges: 184812, Avg degree: 77.375759, Directed:True\n",
            "1.218156s taken for generating adj matrix\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
            "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
            "Q, R = torch.qr(A, some)\n",
            "should be replaced with\n",
            "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:1980.)\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some teleport probabilities are out of range!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(\"negative sample rate:\")\\nprint(sorted(samerates, reverse=True))'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "seed = set_seed(seed)\n",
        "if data == 'BlogCatalog':\n",
        "  num_label = 39\n",
        "elif data == 'wiki':\n",
        "  num_label = 40\n",
        "elif data == \"tweibo\":\n",
        "    num_label = 100\n",
        "elif data == \"orkut\":\n",
        "    num_label = 100\n",
        "else:\n",
        "  raise Exception(\"A wrong graph name!\")\n",
        "\n",
        "n, _, directed = load_info(data)\n",
        "\n",
        "'''if sample_sup_label_set == True:\n",
        "  sup_label_set(data,n)\n",
        "alphafile = \"Lemane/alpha/\" + data + \"_class.txt\"'''\n",
        "\n",
        "G, adj, m = load_edge(data, n, directed, task)\n",
        "\n",
        "cudaid = \"cuda:\" + str(0) if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(cudaid)\n",
        "identity = torch.eye(sample_size,dtype=torch.float64).to(device)\n",
        "\n",
        "Node_Label = load_label(data, n)\n",
        "\n",
        "threshold = 1e-5\n",
        "model = ComputeProximity4SVD(ngraph=sample_size, niter=nhop, dist=dist, param=param, nclass=num_label).to(device)\n",
        "optimizer = torch.optim.SGD([{'params':model.params1,'weight_decay':wdecay1}],lr=lr)\n",
        "\n",
        "best_dist = []\n",
        "best_dist.append(model.params1.clone().cpu().detach().numpy())\n",
        "min_loss = 999999999\n",
        "best_epoch = 0\n",
        "bad_count = 0\n",
        "length_flag = True\n",
        "train_begin = time.time()\n",
        "\n",
        "from collections import defaultdict\n",
        "f = open(\"Lemane/label/\" + data + \".txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "\n",
        "\"\"\"counts = defaultdict(set)\n",
        "labels = defaultdict(set)\n",
        "for line in lines:\n",
        "  vec = line.strip().split()\n",
        "  key = int(vec[0])\n",
        "  for item in vec[1:]:\n",
        "    counts[key].add(int(item))\n",
        "    labels[int(item)].add(key)\n",
        "\n",
        "total = 0\n",
        "for key in labels:\n",
        "  total += len(labels[key])\n",
        "\n",
        "globaldistribution = {}\n",
        "for key in labels:\n",
        "  globaldistribution[key] = len(labels[key]) / total\n",
        "\n",
        "globaldistribution = sorted(globaldistribution.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "print(\"global top 5 labels:\")\n",
        "print(globaldistribution[:5])\n",
        "\n",
        "sortedlabel = []\n",
        "for item in globaldistribution:\n",
        "  sortedlabel.append(item[0])\n",
        "print(\"global sorted labels:\")\n",
        "print(sortedlabel)\n",
        "\n",
        "samerates = []\"\"\"\n",
        "\n",
        "for epoch in range(nepoch):\n",
        "  begin_time = time.time()\n",
        "\n",
        "  bfs_sample_node_list = bfs_sampling(G, sample_size)\n",
        "  #neighbor_sample_node_list = neighbor_sampling(G, sample_size)\n",
        "  #dfs_sample_node_list = dfs_sampling(G, sample_size)\n",
        "  #node2vec_sample_node_list = node2vec_sampling(G, sample_size)\n",
        "  \n",
        "\n",
        "  samplelist = bfs_sample_node_list\n",
        "\n",
        "  prob = get_trans_prob_mat(adj, samplelist, sample_size, task).to(device)\n",
        "  Lap_neg, adj_neg = get_sample_neg_laplacian(sample, len(samplelist))\n",
        "  Lap_neg = Lap_neg.to(device)\n",
        "  Label_list, node_Label = get_sample_label_list(Node_Label, samplelist, num_label)\n",
        "\n",
        "\n",
        "  \"\"\"samplelabels = defaultdict(set)\n",
        "  for node in samplelist:\n",
        "    for la in counts[node]:\n",
        "      samplelabels[la].add(node)\n",
        "\n",
        "  total = 0\n",
        "  for key in samplelabels:\n",
        "    total += len(samplelabels[key])\n",
        "\n",
        "  localdistribution = {}\n",
        "  for key in samplelabels:\n",
        "    localdistribution[key] = len(samplelabels[key]) / total\n",
        "\n",
        "  localdistribution = sorted(localdistribution.items(), key=lambda x:x[1], reverse=True)\n",
        "  print(\"epoch: \" + str(epoch))\n",
        "  print(\"local top 5 labels:\")\n",
        "  print(localdistribution[:5])\n",
        "\n",
        "  sortedlabel = []\n",
        "  for item in localdistribution:\n",
        "    sortedlabel.append(item[0])\n",
        "  print(\"local sorted labels:\")\n",
        "  print(sortedlabel)\n",
        "\n",
        "  ind = 0\n",
        "  index2node = {}\n",
        "  for node in samplelist:\n",
        "    index2node[ind] = node\n",
        "    ind += 1\n",
        "\n",
        "\n",
        "  samelabelnum = 0\n",
        "  adjmatrix = adj_neg.todense()\n",
        "  for i in range(len(samplelist)):\n",
        "    for j in range(len(samplelist)):\n",
        "      if adjmatrix[i, j]:\n",
        "        node1 = index2node[i]\n",
        "        node2 = index2node[j]\n",
        "        if counts[node1] & counts[node2]:\n",
        "          samelabelnum += 1\n",
        "\n",
        "  samerates.append(samelabelnum / np.sum(adjmatrix))\"\"\"\n",
        "\n",
        "\n",
        "  Lap_label = get_label_laplacian(Label_list)\n",
        "  \n",
        "  for i in range(len(Lap_label)):\n",
        "    Lap_label[i] = Lap_label[i].to(device)\n",
        "\n",
        "  loss_train, temp_dist, length_flag = train()\n",
        "  if length_flag == False:\n",
        "    break\n",
        "  if loss_train < min_loss:\n",
        "    min_loss = loss_train\n",
        "    best_epoch = epoch+1\n",
        "    best_dist.append(temp_dist)\n",
        "    bad_count = 0\n",
        "  else:\n",
        "    bad_count += 1\n",
        "\n",
        "  if(epoch+1)%10 == 0:\n",
        "    print('Epoch:{:03d}'.format(epoch+1),'train_loss:{:.5f}'.format(loss_train),'time_spent:{:.5f}s'.format(time.time() - begin_time))\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "  if bad_count == patience:\n",
        "    break\n",
        "\n",
        "\"\"\"print(\"negative sample rate:\")\n",
        "print(sorted(samerates, reverse=True))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jilUjN9pJGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19bf48a7-44fd-486e-c1a0-16f802b8062f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "Training time cost: 67.0506s\n",
            "Best epoch:4th\n",
            "Best distribution: [0.007231   0.03295404 0.08826171 0.16038287 0.23866818 0.31352434\n",
            " 0.38063343 0.43905137 0.48929756 0.53242423 0.56953991 0.60164604\n",
            " 0.62958962 0.65406759 0.67564693 0.69478766]\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "print(\"----------------------------------------------------------------\")\n",
        "print(\"Training time cost: {:.4f}s\".format(time.time() - train_begin))\n",
        "print(\"Best epoch:{}th\".format(best_epoch))\n",
        "print(\"Best distribution: {}\".format(best_dist[-1]))\n",
        "print(\"----------------------------------------------------------------\")\n",
        "if not os.path.exists(\"alpha\"):\n",
        "  os.mkdir(\"alpha\")\n",
        "alphafile = \"alpha/\" + data + \"_class.txt\"\n",
        "np.savetxt(alphafile, best_dist[-1],fmt=\"%.4f\",delimiter=\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Learning Based Proximity Matrix Factorization for Node Embedding",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}