{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scalable and Eﬀective Bipartite Network Embedding",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**$GEBE^p$**"
      ],
      "metadata": {
        "id": "qjycDz7sibxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a paper aiming to use matrix fractionation to speed up the process of PMF(a general form of PPR).\n",
        "\n",
        "The general formula is $H[u_i, u_j]=∑_{l=0}^τω(l)(WW^T)^l[u_i, u_j]$. The three popular choices of $ω(l)$ are uniform distribution applied in uniform high-order proximity: $H_τ=∑_{l=0}^τ\\frac{1}{τ}(WW^T)^l$, Geometric distribution used in PPR: $H_α=∑_{l=0}^τα(1-α)^l(WW^T)^l$, Poisson distribution used in HKPR: $H_λ=∑_{l=0}^τ\\frac{e^{-λ}λ^l}{l!}(WW^T)^l$.\n",
        "\n",
        "However, in the experiments of the authors, they found the Poisson distribution always performed better, so they decide to focus on it and speed it up.\n",
        "\n",
        "For any real square matrix M, we can get $e^M = ∑_{k=0}^∞\\frac{M^k}{k!}$. If we let $M=λWW^T$, we will have $\\frac{e^{λWW^T}}{e^λ}=∑_{l=0}^∞\\frac{e^{-λ}λ^l}{l!}(WW^T)^l=H_λ$, which is exactly the fomula of Poisson distribution used in HKPR.\n",
        "\n",
        "In this case, $e^λH_λ=e^{λWW^T}=e^{λ(ΦΣΨ^T)(ΦΣΨ^T)^T}=e^{λΦΣ^2Φ^T}=Φe^{λΣ^2}Φ^T$, where $ΦΦ^T=I$\n",
        "\n",
        "And then, we get $H_λΦ[·,i]=e^{-λ}Φe^{λΣ^2}Φ^TΦ[·,i]=e^{-λ}e^{λΣ[i,i]^2}Φ[·,i]$, where $e^{-λ}e^{λΣ[i,i]^2}$ is an eigenvalue of $H_λ$ and Φ[·,i] is its corresponding eigenvector.\n",
        "\n",
        "In this case, only use a SVD and some matrix multiplication, we could get the proximity of a square matrix."
      ],
      "metadata": {
        "id": "SnghhGq5ipsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsT5UPlCthot",
        "outputId": "e8d8d83c-c3ac-4159-9cd0-e4abb3ea9245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'BiNE' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/clhchtcjj/BiNE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "EZD2wuswAZ32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "  f = open(path, 'r')\n",
        "  lines = f.readlines()\n",
        "  f.close()\n",
        "  edges = set()\n",
        "  for line in lines:\n",
        "    temp = line.strip().split()\n",
        "    node1 = temp[0]\n",
        "    node2 = temp[1]\n",
        "    weight = float(temp[2])\n",
        "    edges.add((node1, node2, weight))\n",
        "  return edges"
      ],
      "metadata": {
        "id": "GIRYQi4pw9SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainfile = 'BiNE/data/wiki/rating_train.dat'\n",
        "testfile = 'BiNE/data/wiki/ratindg_test.dat'\n",
        "\n",
        "#trainfile = 'BiNE/data/dblp/rating_train.dat'\n",
        "#testfile = 'BiNE/data/dblp/rating_test.dat'\n",
        "\n",
        "trainedges = load_data(trainfile)\n",
        "testedges = load_data(testfile)\n",
        "\n",
        "unode = {}\n",
        "ucount = 0\n",
        "vnode = {}\n",
        "vcount = 0\n",
        "\n",
        "\n",
        "posedges = set()\n",
        "for edges in trainedges:\n",
        "  if edges[0] not in unode:\n",
        "    unode[edges[0]] = ucount\n",
        "    ucount += 1\n",
        "  if edges[1] not in vnode:\n",
        "    vnode[edges[1]] = vcount\n",
        "    vcount += 1\n",
        "  posedges.add((unode[edges[0]], vnode[edges[1]]))\n",
        "\n",
        "for edges in testedges:\n",
        "  if edges[0] not in unode:\n",
        "    unode[edges[0]] = ucount\n",
        "    ucount += 1\n",
        "  if edges[1] not in vnode:\n",
        "    vnode[edges[1]] = vcount\n",
        "    vcount += 1\n",
        "  posedges.add((unode[edges[0]], vnode[edges[1]]))\n",
        "\n",
        "\n",
        "rows = []\n",
        "cols = []\n",
        "weights = []\n",
        "for edges in trainedges:\n",
        "  rows.append(unode[edges[0]])\n",
        "  cols.append(vnode[edges[1]])\n",
        "  weights.append(edges[2])\n",
        "\n",
        "rows = np.array(rows)\n",
        "cols = np.array(cols)\n",
        "weights = np.array(weights)\n",
        "\n",
        "dim = 128\n",
        "lamb = 1\n",
        "\n",
        "adj = sp.csr_matrix((weights, (rows, cols)), shape=(len(unode), len(vnode))).todense()\n",
        "adj = torch.tensor(adj)\n",
        "adj = torch.nn.functional.normalize(adj, dim=0)\n",
        "U, S, V = torch.svd(adj)\n",
        "S = torch.nn.functional.normalize(S, dim=0)\n",
        "U = U[:, :dim]\n",
        "S = S[:dim]\n",
        "V = V[:dim, :]\n",
        "\n",
        "\n",
        "L = math.exp(-lamb / 2) * torch.exp(lamb * S @ S.T / 2)\n",
        "Z = U\n",
        "\n",
        "U = Z * L\n",
        "V = adj.T @ U\n",
        "\n",
        "\n",
        "negedges = set()\n",
        "while len(negedges) < len(testedges) + len(posedges):\n",
        "  node1 = random.randint(0, len(unode) - 1)\n",
        "  node2 = random.randint(0, len(vnode) - 1)\n",
        "  if (node1, node2) not in posedges:\n",
        "    negedges.add((node1, node2))\n",
        "\n",
        "negedges = list(negedges)\n",
        "trainnegedges = negedges[:len(posedges)]\n",
        "testnegedges = negedges[len(posedges):]\n",
        "\n",
        "\n",
        "trainx = []\n",
        "trainy = []\n",
        "\n",
        "testx = []\n",
        "testy = []\n",
        "\n",
        "for edge in trainedges:\n",
        "  trainx.append(torch.cat([U[unode[edge[0]],:], V[vnode[edge[1]],:]], dim=0).numpy().tolist())\n",
        "  trainy.append(1)\n",
        "\n",
        "for edge in trainnegedges:\n",
        "  trainx.append(torch.cat([U[edge[0],:], V[edge[1],:]], dim=0).numpy().tolist())\n",
        "  trainy.append(0)\n",
        "\n",
        "for edge in testedges:\n",
        "  testx.append(torch.cat([U[unode[edge[0]],:], V[vnode[edge[1]],:]], dim=0).numpy().tolist())\n",
        "  testy.append(1)\n",
        "\n",
        "for edge in testnegedges:\n",
        "  testx.append(torch.cat([U[edge[0],:], V[edge[1],:]], dim=0).numpy().tolist())\n",
        "  testy.append(0)\n",
        "\n",
        "trainx = np.array(trainx)\n",
        "trainy = np.array(trainy)\n",
        "testx = np.array(testx)\n",
        "testy = np.array(testy)\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=10000)\n",
        "lr.fit(trainx,trainy)\n",
        "y_pred = lr.predict(testx)\n",
        "auc_lr = roc_auc_score(testy,y_pred)\n",
        "ap_lr = average_precision_score(testy,y_pred)\n",
        "print(\"AUC: {}, AP: {}\".format(auc_lr, ap_lr))"
      ],
      "metadata": {
        "id": "cv4b1m52vbCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87aedfe4-04be-4369-cb8c-7a39ccf2c1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.8635660865813319, AP: 0.8483303401903686\n"
          ]
        }
      ]
    }
  ]
}