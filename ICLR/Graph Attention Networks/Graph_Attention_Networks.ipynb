{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Attention Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GAT**"
      ],
      "metadata": {
        "id": "w63UxKE-dzr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT is an graph representation learning using **attention** mechanism.\n",
        "\n",
        "As we know, we usually use weights on edges to represent the importance of neighbors to center node. However, this fixed weight mechanism may not actually capture the potential relation between two linked nodes, so the author proposed a graph representation learning algorithm using attention mechanism to calculate the importance between them.(Basically, this paper use the same idea with the original attention paper on graph data).\n",
        "\n",
        "First, they **project** the features of two nodes to a lower dimension to reduce the running time and space.\n",
        "\n",
        "Second, they use a **shared attentional mechanism**(a **linear** layer) to calculate **attention coefficients** between each two nodes.\n",
        "\n",
        "Third, to make these coefficients easily **comparable** accross different nodes, they **normalize** them by using **softmax**. And also, to add nonlinearity to the result, they first apply **LeakyReLU** to the coefficients before applying softmax to them.\n",
        "\n",
        "Fourth, from step 1 to 3 would be a attention head. In practice, **multi-head attention** would help to capture different information between each two nodes, so they **average** results of k heads to get the representation of nodes."
      ],
      "metadata": {
        "id": "qCvavAyudz9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdXLy6S0_2si",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cad644d-fd30-4f3e-9408-6698d0cff821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 4.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=9381528fa18ee428d611a95f69a0ca2ad21ab8e98e98dacb05dfe881e72e88f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VDyrvaZ7__kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "nodenum = dataset.data.num_nodes\n",
        "adj = torch.zeros(nodenum, nodenum)\n",
        "edges = dataset.data.edge_index.T\n",
        "for edge in edges:\n",
        "  adj[edge[0]][edge[1]] += 1\n",
        "  adj[edge[1]][edge[0]] += 1\n",
        "print(adj)"
      ],
      "metadata": {
        "id": "0YOJ0Q-fABzQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf1dba9-acec-4207-cc37-c5404b408064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 2.,  ..., 0., 0., 0.],\n",
            "        [0., 2., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 2.],\n",
            "        [0., 0., 0.,  ..., 0., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = dataset[0].to(device)\n",
        "features = data.x\n",
        "print(features.shape)"
      ],
      "metadata": {
        "id": "l7v6mT0YAKqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7977c8a0-c53e-4f16-a7ab-a0c6495cff1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2708, 1433])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xs = torch.IntTensor(nodenum, 1)\n",
        "for i in range(nodenum):\n",
        "  xs[i] = i"
      ],
      "metadata": {
        "id": "NOroMnYsU0Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsz = 64\n",
        "trainset = torch.utils.data.TensorDataset(xs[data.train_mask].to(device), data.y[data.train_mask])\n",
        "train_loader = DataLoader(trainset, batch_size=batchsz, shuffle=True)"
      ],
      "metadata": {
        "id": "zOKatBYQU2I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement this algorithm(train a batch of nodes each time), we first get a batch of nodes. The dimension would be **[batchsz, featuresz]**.\n",
        "\n",
        "And then, since we use a **shared projection** layer to project all nodes to a lower dimension, we pass the feature tensor through **lineartrans** layer and get a tensor with dimension **[batchsz, hiddendim]**\n",
        "\n",
        "They use a **shared attentional mechanism** layer to get the attention coefficients using this formula: $att(Wh_i||Wh_j)$. The att is a linear layer with **[2 * hiddendim, 1]** weight. The input is a **concatenation** of two projected representation of two linked nodes, we could know the coefficient $e_{ij}$ would be **weighted** sum of each element in $Wh_i$ and $Wh_j$. In this case, I use two linear layer att1 and att2 with **[hiddendim, 1]** separately. First, I pass hidden features of all nodes(**[nodenum, hiddendim]**) through att1 and then get a vector **[nodenum, 1]**, where each row is weighted sum of ith node's hidden feature. Then, to get the matrix of coefficients(elements are $e_{ij}$, where i, j is from 1 to nodenum), I use a all ones matrix with dimension **[nodenum, 1]** to multiply with the **transpose** of the vector we get in the first step. After this operation, we get a **[nodenum, nodenum]** matrix, where the ith element of each row is the weighted sum of ith node's hidden feature. After that, I use att2 to get a **[nodenum, 1]** vector again(the meaning of each row is the same as the first one) and use it to add with the **[nodenum, nodenum]** matrix, so for the element in the ith row and j column of this final matrix, it would represent $att(Wh_i||Wh_j)$.\n",
        "\n",
        "Since not all two nodes have links between each other, we use each element in **adjecency matrix** to multiply with according element with the final matrix we just get to get the final coefficient matrix E.\n",
        "\n",
        "Finally, we just perform operation accordingly to get the **normalized** coefficient matrix."
      ],
      "metadata": {
        "id": "ob-FW-YHgmx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, features, adj, in_dim, hidden_dim, out_dim, k):\n",
        "    super(GAT, self).__init__()\n",
        "    self.features = features\n",
        "    self.adj = adj.to(device)\n",
        "    self.k = k\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lineartrans = nn.ModuleList([nn.Linear(in_dim, hidden_dim, bias=False) for i in range(k)])\n",
        "    self.att1 = nn.ModuleList([nn.Linear(hidden_dim, 1, bias=False) for i in range(k)])\n",
        "    self.att2 = nn.ModuleList([nn.Linear(hidden_dim, 1, bias=False) for i in range(k)])\n",
        "    self.mlp = nn.Linear(hidden_dim, out_dim, bias=True)\n",
        "    self.leakyrelu = nn.LeakyReLU()\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.logsoftmax = nn.LogSoftmax()\n",
        "\n",
        "  def forward(self, batch):\n",
        "    batchlist = []\n",
        "    for node in batch:\n",
        "      batchlist.append(node.item())\n",
        "    result = torch.zeros(len(batchlist), self.hidden_dim).to(device)\n",
        "    for i in range(self.k):\n",
        "      hiddenfeatures = self.lineartrans[i](self.features)\n",
        "      att1 = self.att1[i](hiddenfeatures)\n",
        "      att2 = self.att2[i](hiddenfeatures[batchlist])\n",
        "      tmp = torch.ones(len(batchlist), att1.shape[1]).to(device)\n",
        "      tmp = torch.mm(tmp, att1.t())\n",
        "      tmp += att2\n",
        "      e = torch.mul(tmp, self.adj[batchlist])\n",
        "      e = self.leakyrelu(e)\n",
        "      a = self.softmax(e)\n",
        "      out = torch.mm(a, hiddenfeatures)\n",
        "      out = self.leakyrelu(out)\n",
        "      result += out\n",
        "    result /= self.k\n",
        "    result = self.mlp(result)\n",
        "    return self.logsoftmax(result)"
      ],
      "metadata": {
        "id": "w3j0H-5vAzL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "epochs = 50\n",
        "hidden_dim = 256"
      ],
      "metadata": {
        "id": "GDswnLQmYflX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GAT(features, adj, dataset.num_node_features, hidden_dim, dataset.num_classes, 3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "hA9ro6d-YuaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  acc = 0\n",
        "  for x, y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, pred = out.max(dim=1)\n",
        "    acc += float(pred.eq(y).sum().item())\n",
        "  print(\"epoch: {0}, loss: {1}, train acc: {2}\".format(epoch, loss.item(), acc / data.train_mask.sum().item()))"
      ],
      "metadata": {
        "id": "JjDiXAXaY54s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76e4a213-be56-4a49-9790-45ea9acf10ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 12.028645515441895, train acc: 0.08571428571428572\n",
            "epoch: 1, loss: 2.7495124340057373, train acc: 0.17857142857142858\n",
            "epoch: 2, loss: 4.763061046600342, train acc: 0.11428571428571428\n",
            "epoch: 3, loss: 2.6664645671844482, train acc: 0.2571428571428571\n",
            "epoch: 4, loss: 3.093618631362915, train acc: 0.40714285714285714\n",
            "epoch: 5, loss: 1.3721275329589844, train acc: 0.65\n",
            "epoch: 6, loss: 0.35978391766548157, train acc: 0.75\n",
            "epoch: 7, loss: 0.15279605984687805, train acc: 0.85\n",
            "epoch: 8, loss: 0.04930971562862396, train acc: 0.9642857142857143\n",
            "epoch: 9, loss: 0.06350747495889664, train acc: 0.9571428571428572\n",
            "epoch: 10, loss: 0.001685184775851667, train acc: 0.9642857142857143\n",
            "epoch: 11, loss: 4.711748260888271e-05, train acc: 0.9857142857142858\n",
            "epoch: 12, loss: 0.0016544405370950699, train acc: 0.9785714285714285\n",
            "epoch: 13, loss: 0.000497175904456526, train acc: 0.9928571428571429\n",
            "epoch: 14, loss: 0.0006171417771838605, train acc: 0.9928571428571429\n",
            "epoch: 15, loss: 0.0010183415142819285, train acc: 0.9857142857142858\n",
            "epoch: 16, loss: 0.01380204688757658, train acc: 1.0\n",
            "epoch: 17, loss: 0.0015510255470871925, train acc: 1.0\n",
            "epoch: 18, loss: 6.900812149979174e-05, train acc: 0.9928571428571429\n",
            "epoch: 19, loss: 0.008420358411967754, train acc: 1.0\n",
            "epoch: 20, loss: 2.813175342453178e-05, train acc: 1.0\n",
            "epoch: 21, loss: 5.314649115462089e-06, train acc: 1.0\n",
            "epoch: 22, loss: 6.809751357650384e-05, train acc: 1.0\n",
            "epoch: 23, loss: 2.1159512471058406e-06, train acc: 1.0\n",
            "epoch: 24, loss: 6.642130756517872e-05, train acc: 1.0\n",
            "epoch: 25, loss: 3.500435923342593e-05, train acc: 1.0\n",
            "epoch: 26, loss: 2.245967334602028e-05, train acc: 1.0\n",
            "epoch: 27, loss: 0.00023518128728028387, train acc: 1.0\n",
            "epoch: 28, loss: 2.1267891497700475e-05, train acc: 1.0\n",
            "epoch: 29, loss: 1.8377408196101896e-05, train acc: 1.0\n",
            "epoch: 30, loss: 1.824775245040655e-05, train acc: 1.0\n",
            "epoch: 31, loss: 2.067199420707766e-05, train acc: 1.0\n",
            "epoch: 32, loss: 1.872476059361361e-05, train acc: 1.0\n",
            "epoch: 33, loss: 9.864091225608718e-06, train acc: 1.0\n",
            "epoch: 34, loss: 1.5595322111039422e-05, train acc: 1.0\n",
            "epoch: 35, loss: 1.8675950741453562e-06, train acc: 1.0\n",
            "epoch: 36, loss: 1.7612461306271143e-05, train acc: 1.0\n",
            "epoch: 37, loss: 1.412550136592472e-05, train acc: 1.0\n",
            "epoch: 38, loss: 4.400748366606422e-06, train acc: 1.0\n",
            "epoch: 39, loss: 9.913942449202295e-06, train acc: 1.0\n",
            "epoch: 40, loss: 1.1801362234109547e-05, train acc: 1.0\n",
            "epoch: 41, loss: 2.5796087356866337e-05, train acc: 1.0\n",
            "epoch: 42, loss: 2.6424386305734515e-06, train acc: 1.0\n",
            "epoch: 43, loss: 4.688849458034383e-06, train acc: 1.0\n",
            "epoch: 44, loss: 2.7963098546024412e-05, train acc: 1.0\n",
            "epoch: 45, loss: 2.764502823993098e-05, train acc: 1.0\n",
            "epoch: 46, loss: 1.1860908671224024e-05, train acc: 1.0\n",
            "epoch: 47, loss: 8.711761438462418e-06, train acc: 1.0\n",
            "epoch: 48, loss: 1.3539705832954496e-05, train acc: 1.0\n",
            "epoch: 49, loss: 3.258353899582289e-06, train acc: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torch.utils.data.TensorDataset(xs[data.test_mask].to(device), data.y[data.test_mask])\n",
        "test_loader = DataLoader(testset, batch_size=batchsz, shuffle=True)"
      ],
      "metadata": {
        "id": "pxS0X29dkJqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "acc = 0\n",
        "for x, y in test_loader:\n",
        "  out = model(x)\n",
        "  _, pred = out.max(dim=1)\n",
        "  acc += float(pred.eq(y).sum().item())\n",
        "print(\"test acc: {0}\".format(acc / data.test_mask.sum().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZEaw4bKkNPk",
        "outputId": "4408ced2-5200-409a-a399-c3e0413d74c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test acc: 0.512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    }
  ]
}