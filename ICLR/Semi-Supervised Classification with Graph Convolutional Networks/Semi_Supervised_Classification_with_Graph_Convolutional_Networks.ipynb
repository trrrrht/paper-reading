{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi-Supervised Classification with Graph Convolutional Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**GCN**"
      ],
      "metadata": {
        "id": "BSbrNRwV2zck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the first GCN ralted paper. It proposed a very easy way to realize algiorithm learning graph embedding.\n",
        "\n",
        "To simplify, we only discuss the final form of the formula: $Z=f(X, A)=$$softmax(\\hat A\\ ReLU(\\hat AXW^{(0)})W^{(1)})$, where $\\hat A=\\bar D^{-\\frac{1}{2}}\\bar A\\bar D^{-\\frac{1}{2}}$, $\\bar A=A+I_N$, $\\bar D_{ii}=\\sum_j\\bar A_{ij}$, A is the adjacency matrix of X.\n",
        "\n",
        "Now we explain the formula step by step. First, we should add **self-loop** to each node to form $\\bar A$. Why we should do this? because we learning node embedding through passing information through edges. To get information from the **node itself**, we should add self-loops. In this case, $\\bar A$ means information will pass from neighbors and node itself.\n",
        "\n",
        "Second, why we need $\\hat A$. As we can see, $\\bar D$ is the degree of each node(including self-loop). According to experience from real life, the more relations one person has, the **less influence one** relation could have on him or her, so for each node, we should **divide** its adjacency matrix by the its degree(the adjacency matrix actually represents **weights** of edges between nodes)\n",
        "\n",
        "Third, we multiply $\\hat A$ with X. Why should we do this? To understand this formula, we should know the dimension of $\\hat A$ and X. $\\hat A$ is a **[num of nodes, num of nodes]** dimension vector, and X is **[num of nodes, size of node's feature]** dimension vector. According to matrix multiplication, we use each row of $\\hat A$ to multiply with each column of X to get each row of final result. The ith row of $\\hat A$ is the **normalized weights of edge to node i**, and the jth column of X is the **jth feature** of all nodes. After multiply ith row and jth column, we get the **(i, j)** cell of the final result, and it presents the **ith node's jth feature**(all nodes' jth feature passes through normalized edges to node i). After we get all these done, we get $\\hat X$\n",
        "\n",
        "Fourth, we use $\\hat X$ multiplies with $W^{(0)}$. Also, we should check the dimensions of these two matrix. $\\hat X$ is a **[num of nodes, size of node's feature]** dimension vector, and $W^{(0)}$ is a **[size of node's feature, size of hidden feature]**. This contains the same idea of the third step. We just **project** the feature to another space.\n",
        "\n",
        "Fifth, we use ReLU as **non-Linear** operation to **improve the ability to express information** of the neural network model.\n",
        "\n",
        "Sixth, we **repeat** step three to four once. Actually, if we add more Linear layer, we should repeat more times, but the author of this paper only use two layers.(Too much layer of NN model not necessarily improve the performance of NN model, because it may lead representations of all nodes **to be the same**)\n",
        "\n",
        "Finally, we use **softmax** to calculate the probability of nodes belonging to which label.\n",
        "\n",
        "GCN model will take the entrie graph as input, so it's a **transductive** algorithm."
      ],
      "metadata": {
        "id": "ylOB7GwA2zuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDq2J1LncHyS",
        "outputId": "f2531962-ee94-464e-ceb0-f3b4e91a28fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 1.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=cae3bbd5a66a3b9333da37bd97093b48ebb64ed7d8669d5a92ae298cebb0b879\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCzwcIiFbm5k"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "nodenum = dataset.data.num_nodes\n",
        "A = torch.zeros(nodenum, nodenum)\n",
        "edges = dataset.data.edge_index.T\n",
        "for edge in edges:\n",
        "  A[edge[0]][edge[1]] += 1\n",
        "I = torch.eye(nodenum)\n",
        "A += I\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5SevBs7rVZL",
        "outputId": "5f8814c0-18f1-466c-c205-94e5e916efb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
            "        [0., 0., 0.,  ..., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D = torch.zeros(nodenum, nodenum)\n",
        "for i in range(nodenum):\n",
        "  D[i][i] = torch.pow(sum(A[i]), -0.5)\n",
        "  \n",
        "print(D)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oerNtGD9tlEG",
        "outputId": "0b21abc2-fd2b-4d31-c601-c8cae478a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.4082,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.7071, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4472, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.4472]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ahat = torch.mm(torch.mm(D, A), D)\n",
        "print(Ahat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YX_FqEquu02B",
        "outputId": "01870761-8bb6-4f3b-f47c-71c95430b59b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2500, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.2500, 0.2041,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.2041, 0.1667,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.5000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.2000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2000, 0.2000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self, infeature, hiddenfeature, outfeature, A):\n",
        "    super(GCN, self).__init__()\n",
        "    self.infeature = infeature\n",
        "    self.hiddenfeature = hiddenfeature\n",
        "    self.outfeature = outfeature\n",
        "    self.A = A\n",
        "    self.relu = F.relu\n",
        "    self.softmax = F.log_softmax\n",
        "    self.linear1 = nn.Linear(infeature, hiddenfeature, bias=False)\n",
        "    self.linear2 = nn.Linear(hiddenfeature, outfeature, bias=False)\n",
        "    self.dropout = nn.Dropout()\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.relu(self.linear1(torch.mm(self.A, X)))\n",
        "    X = self.softmax(self.linear2(torch.mm(self.A, X)), dim=1)\n",
        "    return X\n"
      ],
      "metadata": {
        "id": "I7tjvEkzvCdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lr = 0.1\n",
        "epochs = 500"
      ],
      "metadata": {
        "id": "Ams0-T3T6AMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ahat = Ahat.to(device)\n",
        "model = GCN(dataset.num_node_features, 32, dataset.num_classes, Ahat).to(device)\n",
        "data = dataset[0].to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "cO75RE9W55gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  out = model(data.x)\n",
        "  loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  _, pred = out.max(dim=1)\n",
        "  traincorrect = float(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
        "  trainacc = traincorrect / data.train_mask.sum().item()\n",
        "  testcorrect = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "  testacc = testcorrect / data.test_mask.sum().item()\n",
        "  print(\"epoch: {0}, loss: {1}, train acc: {2}, test acc: {3}\".format(epoch, loss.item(), trainacc, testacc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLcrBtP_6VTD",
        "outputId": "0e908cdc-d28f-4b50-a13f-8a2eba214907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 1.944702386856079, train acc: 0.21428571428571427, test acc: 0.212\n",
            "epoch: 1, loss: 1.9435466527938843, train acc: 0.21428571428571427, test acc: 0.221\n",
            "epoch: 2, loss: 1.9423930644989014, train acc: 0.21428571428571427, test acc: 0.225\n",
            "epoch: 3, loss: 1.9412434101104736, train acc: 0.21428571428571427, test acc: 0.236\n",
            "epoch: 4, loss: 1.9401018619537354, train acc: 0.22142857142857142, test acc: 0.246\n",
            "epoch: 5, loss: 1.9389528036117554, train acc: 0.2714285714285714, test acc: 0.251\n",
            "epoch: 6, loss: 1.9377944469451904, train acc: 0.2785714285714286, test acc: 0.256\n",
            "epoch: 7, loss: 1.9366058111190796, train acc: 0.29285714285714287, test acc: 0.269\n",
            "epoch: 8, loss: 1.9353927373886108, train acc: 0.3, test acc: 0.272\n",
            "epoch: 9, loss: 1.9341577291488647, train acc: 0.3, test acc: 0.284\n",
            "epoch: 10, loss: 1.9328957796096802, train acc: 0.3, test acc: 0.29\n",
            "epoch: 11, loss: 1.9316056966781616, train acc: 0.3357142857142857, test acc: 0.299\n",
            "epoch: 12, loss: 1.9302891492843628, train acc: 0.35, test acc: 0.307\n",
            "epoch: 13, loss: 1.928945541381836, train acc: 0.37142857142857144, test acc: 0.322\n",
            "epoch: 14, loss: 1.9275723695755005, train acc: 0.39285714285714285, test acc: 0.337\n",
            "epoch: 15, loss: 1.9261729717254639, train acc: 0.4142857142857143, test acc: 0.355\n",
            "epoch: 16, loss: 1.9247421026229858, train acc: 0.42857142857142855, test acc: 0.36\n",
            "epoch: 17, loss: 1.9232755899429321, train acc: 0.45, test acc: 0.367\n",
            "epoch: 18, loss: 1.9217755794525146, train acc: 0.45714285714285713, test acc: 0.379\n",
            "epoch: 19, loss: 1.9202404022216797, train acc: 0.4928571428571429, test acc: 0.389\n",
            "epoch: 20, loss: 1.918665885925293, train acc: 0.5142857142857142, test acc: 0.392\n",
            "epoch: 21, loss: 1.917049765586853, train acc: 0.5357142857142857, test acc: 0.395\n",
            "epoch: 22, loss: 1.915407657623291, train acc: 0.55, test acc: 0.406\n",
            "epoch: 23, loss: 1.9137325286865234, train acc: 0.5928571428571429, test acc: 0.411\n",
            "epoch: 24, loss: 1.9120210409164429, train acc: 0.6, test acc: 0.422\n",
            "epoch: 25, loss: 1.910282015800476, train acc: 0.6071428571428571, test acc: 0.431\n",
            "epoch: 26, loss: 1.908514380455017, train acc: 0.6142857142857143, test acc: 0.433\n",
            "epoch: 27, loss: 1.9067240953445435, train acc: 0.6357142857142857, test acc: 0.439\n",
            "epoch: 28, loss: 1.9049040079116821, train acc: 0.6428571428571429, test acc: 0.445\n",
            "epoch: 29, loss: 1.9030611515045166, train acc: 0.6428571428571429, test acc: 0.45\n",
            "epoch: 30, loss: 1.901185393333435, train acc: 0.65, test acc: 0.454\n",
            "epoch: 31, loss: 1.8992804288864136, train acc: 0.65, test acc: 0.459\n",
            "epoch: 32, loss: 1.8973428010940552, train acc: 0.6642857142857143, test acc: 0.467\n",
            "epoch: 33, loss: 1.8953733444213867, train acc: 0.6642857142857143, test acc: 0.473\n",
            "epoch: 34, loss: 1.8933711051940918, train acc: 0.6857142857142857, test acc: 0.478\n",
            "epoch: 35, loss: 1.8913347721099854, train acc: 0.6928571428571428, test acc: 0.482\n",
            "epoch: 36, loss: 1.889260172843933, train acc: 0.7, test acc: 0.486\n",
            "epoch: 37, loss: 1.8871564865112305, train acc: 0.7071428571428572, test acc: 0.494\n",
            "epoch: 38, loss: 1.8850258588790894, train acc: 0.7071428571428572, test acc: 0.5\n",
            "epoch: 39, loss: 1.8828521966934204, train acc: 0.7142857142857143, test acc: 0.502\n",
            "epoch: 40, loss: 1.8806473016738892, train acc: 0.7214285714285714, test acc: 0.502\n",
            "epoch: 41, loss: 1.878409743309021, train acc: 0.7214285714285714, test acc: 0.508\n",
            "epoch: 42, loss: 1.8761444091796875, train acc: 0.7214285714285714, test acc: 0.514\n",
            "epoch: 43, loss: 1.8738547563552856, train acc: 0.7285714285714285, test acc: 0.515\n",
            "epoch: 44, loss: 1.8715349435806274, train acc: 0.7285714285714285, test acc: 0.516\n",
            "epoch: 45, loss: 1.869179606437683, train acc: 0.7285714285714285, test acc: 0.518\n",
            "epoch: 46, loss: 1.8667863607406616, train acc: 0.7357142857142858, test acc: 0.527\n",
            "epoch: 47, loss: 1.8643659353256226, train acc: 0.7357142857142858, test acc: 0.531\n",
            "epoch: 48, loss: 1.8619149923324585, train acc: 0.7357142857142858, test acc: 0.538\n",
            "epoch: 49, loss: 1.8594292402267456, train acc: 0.7428571428571429, test acc: 0.541\n",
            "epoch: 50, loss: 1.856912612915039, train acc: 0.7428571428571429, test acc: 0.543\n",
            "epoch: 51, loss: 1.854362964630127, train acc: 0.7428571428571429, test acc: 0.546\n",
            "epoch: 52, loss: 1.851778268814087, train acc: 0.7428571428571429, test acc: 0.549\n",
            "epoch: 53, loss: 1.849155068397522, train acc: 0.7428571428571429, test acc: 0.554\n",
            "epoch: 54, loss: 1.8464984893798828, train acc: 0.7428571428571429, test acc: 0.556\n",
            "epoch: 55, loss: 1.8438055515289307, train acc: 0.7428571428571429, test acc: 0.559\n",
            "epoch: 56, loss: 1.8410768508911133, train acc: 0.7428571428571429, test acc: 0.559\n",
            "epoch: 57, loss: 1.838313102722168, train acc: 0.75, test acc: 0.56\n",
            "epoch: 58, loss: 1.8355107307434082, train acc: 0.7571428571428571, test acc: 0.562\n",
            "epoch: 59, loss: 1.8326694965362549, train acc: 0.7571428571428571, test acc: 0.564\n",
            "epoch: 60, loss: 1.829782485961914, train acc: 0.7571428571428571, test acc: 0.564\n",
            "epoch: 61, loss: 1.8268524408340454, train acc: 0.7642857142857142, test acc: 0.566\n",
            "epoch: 62, loss: 1.8238788843154907, train acc: 0.7642857142857142, test acc: 0.571\n",
            "epoch: 63, loss: 1.820863962173462, train acc: 0.7714285714285715, test acc: 0.569\n",
            "epoch: 64, loss: 1.8178071975708008, train acc: 0.7714285714285715, test acc: 0.572\n",
            "epoch: 65, loss: 1.8147073984146118, train acc: 0.7785714285714286, test acc: 0.575\n",
            "epoch: 66, loss: 1.811563491821289, train acc: 0.7785714285714286, test acc: 0.577\n",
            "epoch: 67, loss: 1.808375597000122, train acc: 0.7785714285714286, test acc: 0.583\n",
            "epoch: 68, loss: 1.8051422834396362, train acc: 0.7785714285714286, test acc: 0.584\n",
            "epoch: 69, loss: 1.8018641471862793, train acc: 0.7785714285714286, test acc: 0.585\n",
            "epoch: 70, loss: 1.7985402345657349, train acc: 0.7785714285714286, test acc: 0.588\n",
            "epoch: 71, loss: 1.795173168182373, train acc: 0.7785714285714286, test acc: 0.588\n",
            "epoch: 72, loss: 1.7917592525482178, train acc: 0.7857142857142857, test acc: 0.59\n",
            "epoch: 73, loss: 1.788299322128296, train acc: 0.7928571428571428, test acc: 0.591\n",
            "epoch: 74, loss: 1.784791111946106, train acc: 0.7928571428571428, test acc: 0.591\n",
            "epoch: 75, loss: 1.7812334299087524, train acc: 0.7928571428571428, test acc: 0.592\n",
            "epoch: 76, loss: 1.777629017829895, train acc: 0.7928571428571428, test acc: 0.594\n",
            "epoch: 77, loss: 1.7739741802215576, train acc: 0.8, test acc: 0.593\n",
            "epoch: 78, loss: 1.7702699899673462, train acc: 0.8, test acc: 0.594\n",
            "epoch: 79, loss: 1.7665129899978638, train acc: 0.8, test acc: 0.595\n",
            "epoch: 80, loss: 1.7627047300338745, train acc: 0.8, test acc: 0.596\n",
            "epoch: 81, loss: 1.758844017982483, train acc: 0.8, test acc: 0.596\n",
            "epoch: 82, loss: 1.7549312114715576, train acc: 0.8071428571428572, test acc: 0.597\n",
            "epoch: 83, loss: 1.7509623765945435, train acc: 0.8071428571428572, test acc: 0.599\n",
            "epoch: 84, loss: 1.7469393014907837, train acc: 0.8071428571428572, test acc: 0.601\n",
            "epoch: 85, loss: 1.7428585290908813, train acc: 0.8071428571428572, test acc: 0.601\n",
            "epoch: 86, loss: 1.7387205362319946, train acc: 0.8071428571428572, test acc: 0.602\n",
            "epoch: 87, loss: 1.7345257997512817, train acc: 0.8142857142857143, test acc: 0.602\n",
            "epoch: 88, loss: 1.7302768230438232, train acc: 0.8142857142857143, test acc: 0.603\n",
            "epoch: 89, loss: 1.7259703874588013, train acc: 0.8142857142857143, test acc: 0.607\n",
            "epoch: 90, loss: 1.7216060161590576, train acc: 0.8142857142857143, test acc: 0.607\n",
            "epoch: 91, loss: 1.7171833515167236, train acc: 0.8142857142857143, test acc: 0.609\n",
            "epoch: 92, loss: 1.712697982788086, train acc: 0.8142857142857143, test acc: 0.609\n",
            "epoch: 93, loss: 1.7081531286239624, train acc: 0.8142857142857143, test acc: 0.61\n",
            "epoch: 94, loss: 1.7035456895828247, train acc: 0.8142857142857143, test acc: 0.609\n",
            "epoch: 95, loss: 1.6988762617111206, train acc: 0.8142857142857143, test acc: 0.611\n",
            "epoch: 96, loss: 1.694143295288086, train acc: 0.8142857142857143, test acc: 0.618\n",
            "epoch: 97, loss: 1.6893482208251953, train acc: 0.8142857142857143, test acc: 0.618\n",
            "epoch: 98, loss: 1.6844907999038696, train acc: 0.8142857142857143, test acc: 0.62\n",
            "epoch: 99, loss: 1.6795704364776611, train acc: 0.8142857142857143, test acc: 0.621\n",
            "epoch: 100, loss: 1.6745843887329102, train acc: 0.8142857142857143, test acc: 0.623\n",
            "epoch: 101, loss: 1.6695353984832764, train acc: 0.8142857142857143, test acc: 0.626\n",
            "epoch: 102, loss: 1.664420485496521, train acc: 0.8142857142857143, test acc: 0.627\n",
            "epoch: 103, loss: 1.6592398881912231, train acc: 0.8142857142857143, test acc: 0.63\n",
            "epoch: 104, loss: 1.6539933681488037, train acc: 0.8142857142857143, test acc: 0.633\n",
            "epoch: 105, loss: 1.6486785411834717, train acc: 0.8142857142857143, test acc: 0.633\n",
            "epoch: 106, loss: 1.6432979106903076, train acc: 0.8142857142857143, test acc: 0.636\n",
            "epoch: 107, loss: 1.6378477811813354, train acc: 0.8142857142857143, test acc: 0.637\n",
            "epoch: 108, loss: 1.6323301792144775, train acc: 0.8142857142857143, test acc: 0.64\n",
            "epoch: 109, loss: 1.626743197441101, train acc: 0.8214285714285714, test acc: 0.642\n",
            "epoch: 110, loss: 1.6210858821868896, train acc: 0.8285714285714286, test acc: 0.644\n",
            "epoch: 111, loss: 1.615360140800476, train acc: 0.8357142857142857, test acc: 0.649\n",
            "epoch: 112, loss: 1.609565019607544, train acc: 0.8357142857142857, test acc: 0.652\n",
            "epoch: 113, loss: 1.6036993265151978, train acc: 0.8357142857142857, test acc: 0.655\n",
            "epoch: 114, loss: 1.5977627038955688, train acc: 0.8357142857142857, test acc: 0.66\n",
            "epoch: 115, loss: 1.5917562246322632, train acc: 0.8428571428571429, test acc: 0.665\n",
            "epoch: 116, loss: 1.5856808423995972, train acc: 0.85, test acc: 0.666\n",
            "epoch: 117, loss: 1.5795314311981201, train acc: 0.85, test acc: 0.667\n",
            "epoch: 118, loss: 1.5733133554458618, train acc: 0.85, test acc: 0.669\n",
            "epoch: 119, loss: 1.5670222043991089, train acc: 0.8571428571428571, test acc: 0.67\n",
            "epoch: 120, loss: 1.560661792755127, train acc: 0.8571428571428571, test acc: 0.673\n",
            "epoch: 121, loss: 1.5542296171188354, train acc: 0.8571428571428571, test acc: 0.671\n",
            "epoch: 122, loss: 1.5477287769317627, train acc: 0.8571428571428571, test acc: 0.672\n",
            "epoch: 123, loss: 1.5411555767059326, train acc: 0.8571428571428571, test acc: 0.671\n",
            "epoch: 124, loss: 1.5345135927200317, train acc: 0.8571428571428571, test acc: 0.673\n",
            "epoch: 125, loss: 1.5277998447418213, train acc: 0.8571428571428571, test acc: 0.673\n",
            "epoch: 126, loss: 1.52101731300354, train acc: 0.8571428571428571, test acc: 0.675\n",
            "epoch: 127, loss: 1.5141634941101074, train acc: 0.8642857142857143, test acc: 0.678\n",
            "epoch: 128, loss: 1.5072427988052368, train acc: 0.8642857142857143, test acc: 0.685\n",
            "epoch: 129, loss: 1.500252366065979, train acc: 0.8642857142857143, test acc: 0.685\n",
            "epoch: 130, loss: 1.49319326877594, train acc: 0.8642857142857143, test acc: 0.688\n",
            "epoch: 131, loss: 1.4860655069351196, train acc: 0.8714285714285714, test acc: 0.688\n",
            "epoch: 132, loss: 1.478868842124939, train acc: 0.8714285714285714, test acc: 0.689\n",
            "epoch: 133, loss: 1.4716038703918457, train acc: 0.8714285714285714, test acc: 0.689\n",
            "epoch: 134, loss: 1.464272141456604, train acc: 0.8785714285714286, test acc: 0.693\n",
            "epoch: 135, loss: 1.4568747282028198, train acc: 0.8785714285714286, test acc: 0.694\n",
            "epoch: 136, loss: 1.4494117498397827, train acc: 0.8785714285714286, test acc: 0.695\n",
            "epoch: 137, loss: 1.4418824911117554, train acc: 0.8785714285714286, test acc: 0.697\n",
            "epoch: 138, loss: 1.4342905282974243, train acc: 0.8785714285714286, test acc: 0.701\n",
            "epoch: 139, loss: 1.4266375303268433, train acc: 0.8785714285714286, test acc: 0.7\n",
            "epoch: 140, loss: 1.4189200401306152, train acc: 0.8785714285714286, test acc: 0.7\n",
            "epoch: 141, loss: 1.4111449718475342, train acc: 0.8785714285714286, test acc: 0.706\n",
            "epoch: 142, loss: 1.4033085107803345, train acc: 0.8785714285714286, test acc: 0.711\n",
            "epoch: 143, loss: 1.3954145908355713, train acc: 0.8785714285714286, test acc: 0.715\n",
            "epoch: 144, loss: 1.387465000152588, train acc: 0.8785714285714286, test acc: 0.716\n",
            "epoch: 145, loss: 1.379459023475647, train acc: 0.8714285714285714, test acc: 0.718\n",
            "epoch: 146, loss: 1.3714001178741455, train acc: 0.8785714285714286, test acc: 0.719\n",
            "epoch: 147, loss: 1.3632882833480835, train acc: 0.8785714285714286, test acc: 0.721\n",
            "epoch: 148, loss: 1.3551241159439087, train acc: 0.8785714285714286, test acc: 0.723\n",
            "epoch: 149, loss: 1.3469125032424927, train acc: 0.8857142857142857, test acc: 0.727\n",
            "epoch: 150, loss: 1.338653326034546, train acc: 0.8857142857142857, test acc: 0.729\n",
            "epoch: 151, loss: 1.3303487300872803, train acc: 0.8928571428571429, test acc: 0.731\n",
            "epoch: 152, loss: 1.321998119354248, train acc: 0.8928571428571429, test acc: 0.733\n",
            "epoch: 153, loss: 1.3136091232299805, train acc: 0.8928571428571429, test acc: 0.733\n",
            "epoch: 154, loss: 1.3051813840866089, train acc: 0.8928571428571429, test acc: 0.734\n",
            "epoch: 155, loss: 1.296714186668396, train acc: 0.8928571428571429, test acc: 0.732\n",
            "epoch: 156, loss: 1.2882126569747925, train acc: 0.8928571428571429, test acc: 0.733\n",
            "epoch: 157, loss: 1.2796781063079834, train acc: 0.9, test acc: 0.733\n",
            "epoch: 158, loss: 1.271115779876709, train acc: 0.9071428571428571, test acc: 0.736\n",
            "epoch: 159, loss: 1.262521505355835, train acc: 0.9071428571428571, test acc: 0.737\n",
            "epoch: 160, loss: 1.2539044618606567, train acc: 0.9142857142857143, test acc: 0.738\n",
            "epoch: 161, loss: 1.2452610731124878, train acc: 0.9214285714285714, test acc: 0.739\n",
            "epoch: 162, loss: 1.2365970611572266, train acc: 0.9214285714285714, test acc: 0.739\n",
            "epoch: 163, loss: 1.227916955947876, train acc: 0.9214285714285714, test acc: 0.74\n",
            "epoch: 164, loss: 1.219218373298645, train acc: 0.9214285714285714, test acc: 0.743\n",
            "epoch: 165, loss: 1.2105070352554321, train acc: 0.9214285714285714, test acc: 0.744\n",
            "epoch: 166, loss: 1.2017797231674194, train acc: 0.9214285714285714, test acc: 0.745\n",
            "epoch: 167, loss: 1.1930458545684814, train acc: 0.9214285714285714, test acc: 0.747\n",
            "epoch: 168, loss: 1.1843037605285645, train acc: 0.9214285714285714, test acc: 0.748\n",
            "epoch: 169, loss: 1.1755595207214355, train acc: 0.9214285714285714, test acc: 0.75\n",
            "epoch: 170, loss: 1.1668140888214111, train acc: 0.9214285714285714, test acc: 0.75\n",
            "epoch: 171, loss: 1.1580698490142822, train acc: 0.9142857142857143, test acc: 0.751\n",
            "epoch: 172, loss: 1.1493326425552368, train acc: 0.9142857142857143, test acc: 0.753\n",
            "epoch: 173, loss: 1.1406010389328003, train acc: 0.9142857142857143, test acc: 0.753\n",
            "epoch: 174, loss: 1.1318798065185547, train acc: 0.9142857142857143, test acc: 0.753\n",
            "epoch: 175, loss: 1.1231714487075806, train acc: 0.9142857142857143, test acc: 0.753\n",
            "epoch: 176, loss: 1.1144760847091675, train acc: 0.9142857142857143, test acc: 0.754\n",
            "epoch: 177, loss: 1.1057981252670288, train acc: 0.9142857142857143, test acc: 0.755\n",
            "epoch: 178, loss: 1.0971400737762451, train acc: 0.9142857142857143, test acc: 0.755\n",
            "epoch: 179, loss: 1.0885052680969238, train acc: 0.9142857142857143, test acc: 0.756\n",
            "epoch: 180, loss: 1.0798933506011963, train acc: 0.9142857142857143, test acc: 0.757\n",
            "epoch: 181, loss: 1.0713075399398804, train acc: 0.9142857142857143, test acc: 0.756\n",
            "epoch: 182, loss: 1.0627518892288208, train acc: 0.9142857142857143, test acc: 0.757\n",
            "epoch: 183, loss: 1.0542253255844116, train acc: 0.9142857142857143, test acc: 0.759\n",
            "epoch: 184, loss: 1.0457319021224976, train acc: 0.9142857142857143, test acc: 0.757\n",
            "epoch: 185, loss: 1.0372754335403442, train acc: 0.9214285714285714, test acc: 0.758\n",
            "epoch: 186, loss: 1.0288523435592651, train acc: 0.9214285714285714, test acc: 0.759\n",
            "epoch: 187, loss: 1.0204715728759766, train acc: 0.9214285714285714, test acc: 0.76\n",
            "epoch: 188, loss: 1.012129783630371, train acc: 0.9214285714285714, test acc: 0.761\n",
            "epoch: 189, loss: 1.0038323402404785, train acc: 0.9214285714285714, test acc: 0.762\n",
            "epoch: 190, loss: 0.9955790638923645, train acc: 0.9214285714285714, test acc: 0.763\n",
            "epoch: 191, loss: 0.9873722791671753, train acc: 0.9214285714285714, test acc: 0.763\n",
            "epoch: 192, loss: 0.9792119860649109, train acc: 0.9214285714285714, test acc: 0.763\n",
            "epoch: 193, loss: 0.9711012840270996, train acc: 0.9214285714285714, test acc: 0.763\n",
            "epoch: 194, loss: 0.963042676448822, train acc: 0.9214285714285714, test acc: 0.763\n",
            "epoch: 195, loss: 0.955033540725708, train acc: 0.9285714285714286, test acc: 0.766\n",
            "epoch: 196, loss: 0.9470798969268799, train acc: 0.9285714285714286, test acc: 0.766\n",
            "epoch: 197, loss: 0.9391804337501526, train acc: 0.9285714285714286, test acc: 0.768\n",
            "epoch: 198, loss: 0.93133544921875, train acc: 0.9285714285714286, test acc: 0.769\n",
            "epoch: 199, loss: 0.9235487580299377, train acc: 0.9285714285714286, test acc: 0.769\n",
            "epoch: 200, loss: 0.915818989276886, train acc: 0.9285714285714286, test acc: 0.769\n",
            "epoch: 201, loss: 0.9081474542617798, train acc: 0.9285714285714286, test acc: 0.772\n",
            "epoch: 202, loss: 0.9005367159843445, train acc: 0.9285714285714286, test acc: 0.772\n",
            "epoch: 203, loss: 0.8929865956306458, train acc: 0.9285714285714286, test acc: 0.774\n",
            "epoch: 204, loss: 0.885497510433197, train acc: 0.9285714285714286, test acc: 0.774\n",
            "epoch: 205, loss: 0.8780691623687744, train acc: 0.9285714285714286, test acc: 0.775\n",
            "epoch: 206, loss: 0.8707037568092346, train acc: 0.9357142857142857, test acc: 0.775\n",
            "epoch: 207, loss: 0.8634011149406433, train acc: 0.9357142857142857, test acc: 0.777\n",
            "epoch: 208, loss: 0.8561598658561707, train acc: 0.9357142857142857, test acc: 0.778\n",
            "epoch: 209, loss: 0.8489840626716614, train acc: 0.9357142857142857, test acc: 0.779\n",
            "epoch: 210, loss: 0.8418678641319275, train acc: 0.9357142857142857, test acc: 0.781\n",
            "epoch: 211, loss: 0.8348161578178406, train acc: 0.9357142857142857, test acc: 0.781\n",
            "epoch: 212, loss: 0.8278271555900574, train acc: 0.9357142857142857, test acc: 0.781\n",
            "epoch: 213, loss: 0.8209034204483032, train acc: 0.9428571428571428, test acc: 0.782\n",
            "epoch: 214, loss: 0.8140435218811035, train acc: 0.9428571428571428, test acc: 0.783\n",
            "epoch: 215, loss: 0.8072481751441956, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 216, loss: 0.8005170822143555, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 217, loss: 0.7938503623008728, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 218, loss: 0.787248432636261, train acc: 0.9428571428571428, test acc: 0.785\n",
            "epoch: 219, loss: 0.7807086110115051, train acc: 0.9428571428571428, test acc: 0.785\n",
            "epoch: 220, loss: 0.774235188961029, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 221, loss: 0.7678232192993164, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 222, loss: 0.7614758014678955, train acc: 0.9428571428571428, test acc: 0.786\n",
            "epoch: 223, loss: 0.7551908493041992, train acc: 0.9428571428571428, test acc: 0.787\n",
            "epoch: 224, loss: 0.7489689588546753, train acc: 0.9428571428571428, test acc: 0.787\n",
            "epoch: 225, loss: 0.7428096532821655, train acc: 0.9428571428571428, test acc: 0.787\n",
            "epoch: 226, loss: 0.7367123961448669, train acc: 0.9428571428571428, test acc: 0.788\n",
            "epoch: 227, loss: 0.7306768894195557, train acc: 0.9428571428571428, test acc: 0.788\n",
            "epoch: 228, loss: 0.7247021794319153, train acc: 0.9428571428571428, test acc: 0.788\n",
            "epoch: 229, loss: 0.7187907695770264, train acc: 0.9428571428571428, test acc: 0.789\n",
            "epoch: 230, loss: 0.7129382491111755, train acc: 0.9428571428571428, test acc: 0.789\n",
            "epoch: 231, loss: 0.7071453928947449, train acc: 0.9428571428571428, test acc: 0.79\n",
            "epoch: 232, loss: 0.7014131546020508, train acc: 0.9428571428571428, test acc: 0.791\n",
            "epoch: 233, loss: 0.6957394480705261, train acc: 0.95, test acc: 0.791\n",
            "epoch: 234, loss: 0.6901254653930664, train acc: 0.95, test acc: 0.791\n",
            "epoch: 235, loss: 0.6845701336860657, train acc: 0.95, test acc: 0.792\n",
            "epoch: 236, loss: 0.6790723204612732, train acc: 0.95, test acc: 0.792\n",
            "epoch: 237, loss: 0.6736320853233337, train acc: 0.95, test acc: 0.792\n",
            "epoch: 238, loss: 0.6682481169700623, train acc: 0.9571428571428572, test acc: 0.792\n",
            "epoch: 239, loss: 0.6629221439361572, train acc: 0.9571428571428572, test acc: 0.792\n",
            "epoch: 240, loss: 0.6576502919197083, train acc: 0.9571428571428572, test acc: 0.792\n",
            "epoch: 241, loss: 0.6524350047111511, train acc: 0.9571428571428572, test acc: 0.793\n",
            "epoch: 242, loss: 0.6472737789154053, train acc: 0.9571428571428572, test acc: 0.797\n",
            "epoch: 243, loss: 0.6421661972999573, train acc: 0.9571428571428572, test acc: 0.798\n",
            "epoch: 244, loss: 0.6371139883995056, train acc: 0.9571428571428572, test acc: 0.799\n",
            "epoch: 245, loss: 0.6321144700050354, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 246, loss: 0.6271676421165466, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 247, loss: 0.6222736835479736, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 248, loss: 0.6174291372299194, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 249, loss: 0.612635612487793, train acc: 0.9571428571428572, test acc: 0.801\n",
            "epoch: 250, loss: 0.6078929901123047, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 251, loss: 0.6031995415687561, train acc: 0.9571428571428572, test acc: 0.8\n",
            "epoch: 252, loss: 0.5985561013221741, train acc: 0.9571428571428572, test acc: 0.801\n",
            "epoch: 253, loss: 0.5939602851867676, train acc: 0.9571428571428572, test acc: 0.801\n",
            "epoch: 254, loss: 0.5894126892089844, train acc: 0.9571428571428572, test acc: 0.801\n",
            "epoch: 255, loss: 0.5849122405052185, train acc: 0.9571428571428572, test acc: 0.801\n",
            "epoch: 256, loss: 0.5804595351219177, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 257, loss: 0.5760526657104492, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 258, loss: 0.5716917514801025, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 259, loss: 0.5673758387565613, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 260, loss: 0.5631043314933777, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 261, loss: 0.5588774681091309, train acc: 0.9571428571428572, test acc: 0.802\n",
            "epoch: 262, loss: 0.5546953082084656, train acc: 0.9571428571428572, test acc: 0.803\n",
            "epoch: 263, loss: 0.5505556464195251, train acc: 0.9642857142857143, test acc: 0.803\n",
            "epoch: 264, loss: 0.5464591383934021, train acc: 0.9642857142857143, test acc: 0.803\n",
            "epoch: 265, loss: 0.5424053072929382, train acc: 0.9642857142857143, test acc: 0.803\n",
            "epoch: 266, loss: 0.538392961025238, train acc: 0.9642857142857143, test acc: 0.803\n",
            "epoch: 267, loss: 0.534421443939209, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 268, loss: 0.5304911732673645, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 269, loss: 0.5266003012657166, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 270, loss: 0.5227496027946472, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 271, loss: 0.5189383625984192, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 272, loss: 0.5151647329330444, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 273, loss: 0.5114304423332214, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 274, loss: 0.5077332258224487, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 275, loss: 0.5040740966796875, train acc: 0.9642857142857143, test acc: 0.805\n",
            "epoch: 276, loss: 0.5004510879516602, train acc: 0.9642857142857143, test acc: 0.806\n",
            "epoch: 277, loss: 0.4968649744987488, train acc: 0.9642857142857143, test acc: 0.807\n",
            "epoch: 278, loss: 0.49331414699554443, train acc: 0.9642857142857143, test acc: 0.807\n",
            "epoch: 279, loss: 0.48979976773262024, train acc: 0.9642857142857143, test acc: 0.808\n",
            "epoch: 280, loss: 0.4863201081752777, train acc: 0.9642857142857143, test acc: 0.808\n",
            "epoch: 281, loss: 0.48287609219551086, train acc: 0.9642857142857143, test acc: 0.808\n",
            "epoch: 282, loss: 0.4794655442237854, train acc: 0.9642857142857143, test acc: 0.807\n",
            "epoch: 283, loss: 0.4760899245738983, train acc: 0.9714285714285714, test acc: 0.807\n",
            "epoch: 284, loss: 0.4727478623390198, train acc: 0.9714285714285714, test acc: 0.807\n",
            "epoch: 285, loss: 0.46943849325180054, train acc: 0.9714285714285714, test acc: 0.806\n",
            "epoch: 286, loss: 0.4661621153354645, train acc: 0.9714285714285714, test acc: 0.806\n",
            "epoch: 287, loss: 0.46291863918304443, train acc: 0.9714285714285714, test acc: 0.807\n",
            "epoch: 288, loss: 0.4597075283527374, train acc: 0.9714285714285714, test acc: 0.806\n",
            "epoch: 289, loss: 0.45652610063552856, train acc: 0.9714285714285714, test acc: 0.806\n",
            "epoch: 290, loss: 0.45337623357772827, train acc: 0.9714285714285714, test acc: 0.806\n",
            "epoch: 291, loss: 0.4502570629119873, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 292, loss: 0.4471682012081146, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 293, loss: 0.44410890340805054, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 294, loss: 0.44107937812805176, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 295, loss: 0.4380797743797302, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 296, loss: 0.435108482837677, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 297, loss: 0.4321666359901428, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 298, loss: 0.4292527735233307, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 299, loss: 0.42636600136756897, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 300, loss: 0.4235072731971741, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 301, loss: 0.4206760823726654, train acc: 0.9785714285714285, test acc: 0.806\n",
            "epoch: 302, loss: 0.4178725481033325, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 303, loss: 0.4150942862033844, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 304, loss: 0.4123430550098419, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 305, loss: 0.4096175730228424, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 306, loss: 0.406917542219162, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 307, loss: 0.4042423665523529, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 308, loss: 0.40159207582473755, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 309, loss: 0.3989664316177368, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 310, loss: 0.39636534452438354, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 311, loss: 0.39378821849823, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 312, loss: 0.39123454689979553, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 313, loss: 0.3887055516242981, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 314, loss: 0.38619863986968994, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 315, loss: 0.3837147355079651, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 316, loss: 0.3812534809112549, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 317, loss: 0.37881386280059814, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 318, loss: 0.37639567255973816, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 319, loss: 0.37400010228157043, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 320, loss: 0.37162500619888306, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 321, loss: 0.36927080154418945, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 322, loss: 0.3669382333755493, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 323, loss: 0.36462655663490295, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 324, loss: 0.36233529448509216, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 325, loss: 0.3600645661354065, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 326, loss: 0.3578133285045624, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 327, loss: 0.3555821478366852, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 328, loss: 0.3533717095851898, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 329, loss: 0.3511800169944763, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 330, loss: 0.34900784492492676, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 331, loss: 0.346855491399765, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 332, loss: 0.3447207510471344, train acc: 0.9785714285714285, test acc: 0.807\n",
            "epoch: 333, loss: 0.3426053822040558, train acc: 0.9785714285714285, test acc: 0.808\n",
            "epoch: 334, loss: 0.3405081629753113, train acc: 0.9785714285714285, test acc: 0.809\n",
            "epoch: 335, loss: 0.33842897415161133, train acc: 0.9785714285714285, test acc: 0.81\n",
            "epoch: 336, loss: 0.3363690674304962, train acc: 0.9857142857142858, test acc: 0.811\n",
            "epoch: 337, loss: 0.3343265652656555, train acc: 0.9857142857142858, test acc: 0.811\n",
            "epoch: 338, loss: 0.3323020339012146, train acc: 0.9857142857142858, test acc: 0.81\n",
            "epoch: 339, loss: 0.33029505610466003, train acc: 0.9857142857142858, test acc: 0.812\n",
            "epoch: 340, loss: 0.3283044397830963, train acc: 0.9857142857142858, test acc: 0.811\n",
            "epoch: 341, loss: 0.3263314664363861, train acc: 0.9857142857142858, test acc: 0.811\n",
            "epoch: 342, loss: 0.32437533140182495, train acc: 0.9857142857142858, test acc: 0.811\n",
            "epoch: 343, loss: 0.32243508100509644, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 344, loss: 0.32051217555999756, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 345, loss: 0.31860488653182983, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 346, loss: 0.3167138695716858, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 347, loss: 0.3148389160633087, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 348, loss: 0.31297948956489563, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 349, loss: 0.31113511323928833, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 350, loss: 0.3093068301677704, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 351, loss: 0.30749356746673584, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 352, loss: 0.30569446086883545, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 353, loss: 0.3039091229438782, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 354, loss: 0.30213847756385803, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 355, loss: 0.30038222670555115, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 356, loss: 0.29864153265953064, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 357, loss: 0.29691430926322937, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 358, loss: 0.2952011525630951, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 359, loss: 0.29350295662879944, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 360, loss: 0.2918175160884857, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 361, loss: 0.29014670848846436, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 362, loss: 0.28848952054977417, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 363, loss: 0.2868456244468689, train acc: 0.9857142857142858, test acc: 0.814\n",
            "epoch: 364, loss: 0.28521499037742615, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 365, loss: 0.28359800577163696, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 366, loss: 0.28199416399002075, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 367, loss: 0.28040340542793274, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 368, loss: 0.27882519364356995, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 369, loss: 0.27726122736930847, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 370, loss: 0.275707870721817, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 371, loss: 0.27416834235191345, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 372, loss: 0.2726403772830963, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 373, loss: 0.2711244821548462, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 374, loss: 0.26962095499038696, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 375, loss: 0.26812902092933655, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 376, loss: 0.26664915680885315, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 377, loss: 0.26518088579177856, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 378, loss: 0.2637235224246979, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 379, loss: 0.26227840781211853, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 380, loss: 0.26084384322166443, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 381, loss: 0.2594209611415863, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 382, loss: 0.25800877809524536, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 383, loss: 0.2566078007221222, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 384, loss: 0.2552175223827362, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 385, loss: 0.2538382411003113, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 386, loss: 0.2524695098400116, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 387, loss: 0.25111162662506104, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 388, loss: 0.24976316094398499, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 389, loss: 0.2484264075756073, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 390, loss: 0.24709884822368622, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 391, loss: 0.24578170478343964, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 392, loss: 0.2444748431444168, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 393, loss: 0.2431776374578476, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 394, loss: 0.24189035594463348, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 395, loss: 0.2406131476163864, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 396, loss: 0.23934617638587952, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 397, loss: 0.23808863759040833, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 398, loss: 0.23684044182300568, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 399, loss: 0.23560191690921783, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 400, loss: 0.23437270522117615, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 401, loss: 0.23315268754959106, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 402, loss: 0.23194190859794617, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 403, loss: 0.23073981702327728, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 404, loss: 0.2295466512441635, train acc: 0.9857142857142858, test acc: 0.815\n",
            "epoch: 405, loss: 0.2283630073070526, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 406, loss: 0.22718732059001923, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 407, loss: 0.22602099180221558, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 408, loss: 0.2248634248971939, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 409, loss: 0.22371454536914825, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 410, loss: 0.22257357835769653, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 411, loss: 0.2214413434267044, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 412, loss: 0.22031673789024353, train acc: 0.9857142857142858, test acc: 0.816\n",
            "epoch: 413, loss: 0.21920111775398254, train acc: 0.9857142857142858, test acc: 0.817\n",
            "epoch: 414, loss: 0.21809324622154236, train acc: 0.9857142857142858, test acc: 0.818\n",
            "epoch: 415, loss: 0.2169932872056961, train acc: 0.9857142857142858, test acc: 0.818\n",
            "epoch: 416, loss: 0.2159019559621811, train acc: 0.9857142857142858, test acc: 0.818\n",
            "epoch: 417, loss: 0.21481813490390778, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 418, loss: 0.2137419581413269, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 419, loss: 0.2126740664243698, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 420, loss: 0.21161337196826935, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 421, loss: 0.21056082844734192, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 422, loss: 0.2095155119895935, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 423, loss: 0.20847764611244202, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 424, loss: 0.2074473649263382, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 425, loss: 0.20642507076263428, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 426, loss: 0.20540904998779297, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 427, loss: 0.20440097153186798, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 428, loss: 0.20339998602867126, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 429, loss: 0.20240552723407745, train acc: 0.9928571428571429, test acc: 0.817\n",
            "epoch: 430, loss: 0.2014186978340149, train acc: 0.9928571428571429, test acc: 0.817\n",
            "epoch: 431, loss: 0.20043891668319702, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 432, loss: 0.1994655877351761, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 433, loss: 0.1985001266002655, train acc: 0.9928571428571429, test acc: 0.818\n",
            "epoch: 434, loss: 0.19754092395305634, train acc: 0.9928571428571429, test acc: 0.817\n",
            "epoch: 435, loss: 0.19658809900283813, train acc: 0.9928571428571429, test acc: 0.817\n",
            "epoch: 436, loss: 0.19564253091812134, train acc: 0.9928571428571429, test acc: 0.816\n",
            "epoch: 437, loss: 0.19470328092575073, train acc: 0.9928571428571429, test acc: 0.816\n",
            "epoch: 438, loss: 0.1937701404094696, train acc: 0.9928571428571429, test acc: 0.816\n",
            "epoch: 439, loss: 0.19284436106681824, train acc: 0.9928571428571429, test acc: 0.816\n",
            "epoch: 440, loss: 0.1919243186712265, train acc: 0.9928571428571429, test acc: 0.815\n",
            "epoch: 441, loss: 0.1910109966993332, train acc: 0.9928571428571429, test acc: 0.815\n",
            "epoch: 442, loss: 0.19010421633720398, train acc: 1.0, test acc: 0.816\n",
            "epoch: 443, loss: 0.1892034411430359, train acc: 1.0, test acc: 0.816\n",
            "epoch: 444, loss: 0.18830853700637817, train acc: 1.0, test acc: 0.816\n",
            "epoch: 445, loss: 0.18742038309574127, train acc: 1.0, test acc: 0.816\n",
            "epoch: 446, loss: 0.18653804063796997, train acc: 1.0, test acc: 0.816\n",
            "epoch: 447, loss: 0.18566155433654785, train acc: 1.0, test acc: 0.816\n",
            "epoch: 448, loss: 0.18479180335998535, train acc: 1.0, test acc: 0.816\n",
            "epoch: 449, loss: 0.18392828106880188, train acc: 1.0, test acc: 0.816\n",
            "epoch: 450, loss: 0.1830700933933258, train acc: 1.0, test acc: 0.816\n",
            "epoch: 451, loss: 0.18221816420555115, train acc: 1.0, test acc: 0.816\n",
            "epoch: 452, loss: 0.1813720315694809, train acc: 1.0, test acc: 0.816\n",
            "epoch: 453, loss: 0.1805310845375061, train acc: 1.0, test acc: 0.816\n",
            "epoch: 454, loss: 0.1796964854001999, train acc: 1.0, test acc: 0.816\n",
            "epoch: 455, loss: 0.1788673847913742, train acc: 1.0, test acc: 0.816\n",
            "epoch: 456, loss: 0.17804387211799622, train acc: 1.0, test acc: 0.816\n",
            "epoch: 457, loss: 0.17722658812999725, train acc: 1.0, test acc: 0.816\n",
            "epoch: 458, loss: 0.17641454935073853, train acc: 1.0, test acc: 0.816\n",
            "epoch: 459, loss: 0.1756073385477066, train acc: 1.0, test acc: 0.816\n",
            "epoch: 460, loss: 0.17480617761611938, train acc: 1.0, test acc: 0.817\n",
            "epoch: 461, loss: 0.17401035130023956, train acc: 1.0, test acc: 0.817\n",
            "epoch: 462, loss: 0.1732194423675537, train acc: 1.0, test acc: 0.817\n",
            "epoch: 463, loss: 0.17243431508541107, train acc: 1.0, test acc: 0.817\n",
            "epoch: 464, loss: 0.17165441811084747, train acc: 1.0, test acc: 0.817\n",
            "epoch: 465, loss: 0.17087973654270172, train acc: 1.0, test acc: 0.817\n",
            "epoch: 466, loss: 0.17011000216007233, train acc: 1.0, test acc: 0.817\n",
            "epoch: 467, loss: 0.16934573650360107, train acc: 1.0, test acc: 0.817\n",
            "epoch: 468, loss: 0.16858595609664917, train acc: 1.0, test acc: 0.817\n",
            "epoch: 469, loss: 0.1678316593170166, train acc: 1.0, test acc: 0.817\n",
            "epoch: 470, loss: 0.16708238422870636, train acc: 1.0, test acc: 0.817\n",
            "epoch: 471, loss: 0.1663375347852707, train acc: 1.0, test acc: 0.817\n",
            "epoch: 472, loss: 0.165598064661026, train acc: 1.0, test acc: 0.817\n",
            "epoch: 473, loss: 0.16486388444900513, train acc: 1.0, test acc: 0.817\n",
            "epoch: 474, loss: 0.16413374245166779, train acc: 1.0, test acc: 0.816\n",
            "epoch: 475, loss: 0.16340866684913635, train acc: 1.0, test acc: 0.816\n",
            "epoch: 476, loss: 0.1626884788274765, train acc: 1.0, test acc: 0.816\n",
            "epoch: 477, loss: 0.16197268664836884, train acc: 1.0, test acc: 0.816\n",
            "epoch: 478, loss: 0.161261647939682, train acc: 1.0, test acc: 0.816\n",
            "epoch: 479, loss: 0.16055558621883392, train acc: 1.0, test acc: 0.816\n",
            "epoch: 480, loss: 0.15985390543937683, train acc: 1.0, test acc: 0.816\n",
            "epoch: 481, loss: 0.1591566652059555, train acc: 1.0, test acc: 0.816\n",
            "epoch: 482, loss: 0.15846416354179382, train acc: 1.0, test acc: 0.816\n",
            "epoch: 483, loss: 0.15777622163295746, train acc: 1.0, test acc: 0.816\n",
            "epoch: 484, loss: 0.15709233283996582, train acc: 1.0, test acc: 0.816\n",
            "epoch: 485, loss: 0.15641318261623383, train acc: 1.0, test acc: 0.816\n",
            "epoch: 486, loss: 0.15573856234550476, train acc: 1.0, test acc: 0.816\n",
            "epoch: 487, loss: 0.15506774187088013, train acc: 1.0, test acc: 0.816\n",
            "epoch: 488, loss: 0.15440170466899872, train acc: 1.0, test acc: 0.816\n",
            "epoch: 489, loss: 0.15374016761779785, train acc: 1.0, test acc: 0.816\n",
            "epoch: 490, loss: 0.15308207273483276, train acc: 1.0, test acc: 0.816\n",
            "epoch: 491, loss: 0.15242871642112732, train acc: 1.0, test acc: 0.816\n",
            "epoch: 492, loss: 0.15177929401397705, train acc: 1.0, test acc: 0.816\n",
            "epoch: 493, loss: 0.15113402903079987, train acc: 1.0, test acc: 0.816\n",
            "epoch: 494, loss: 0.15049275755882263, train acc: 1.0, test acc: 0.817\n",
            "epoch: 495, loss: 0.14985568821430206, train acc: 1.0, test acc: 0.817\n",
            "epoch: 496, loss: 0.14922292530536652, train acc: 1.0, test acc: 0.817\n",
            "epoch: 497, loss: 0.14859364926815033, train acc: 1.0, test acc: 0.817\n",
            "epoch: 498, loss: 0.14796854555606842, train acc: 1.0, test acc: 0.817\n",
            "epoch: 499, loss: 0.1473473310470581, train acc: 1.0, test acc: 0.817\n"
          ]
        }
      ]
    }
  ]
}