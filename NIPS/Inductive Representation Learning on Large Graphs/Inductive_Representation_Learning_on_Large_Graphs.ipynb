{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inductive Representation Learning on Large Graphs",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Graphsage**"
      ],
      "metadata": {
        "id": "qeKxN2NIh02H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphsage is a inductive representation learning algorithm for large graphs.\n",
        "\n",
        "Unlike transductive algorithms, which aim to learn the embeddings of each nodes, Graphsage aims to learn an aggregator functions instead.\n",
        "\n",
        "As we know, **unsupervised algorithms** like deepwalk, node2vec aim to generate different walk sequences based on structual information of graph and then use skip-gram to learn the embedding of each node in the graph. These embeddings are not related to specific downstream task, so they could be general to many tasks. However, the ability to express the information containing in the graph is also limited. In this case, some authros proposed **GCN**, which is the first transductive algorithm using NN model. Each epoch, the model will update embeddings for each node. For large graphs, this usually could not be done.\n",
        "\n",
        "Graphsage is designed to fix these flaws in the field of graph representation learning. Instead of directly learning node embeddings for each node, the authors decide to learn **aggregators** for nodes to get information from their neighbors and get the final embeddings.\n",
        "\n",
        "Since we know, if we want to get as much information in the graph as we can, we should consider the structual information to generate embeddings for nodes. How to get **structual information**? Usually, we consider **k-hop neighbors** of nodes as their k-level structual information. Graphsage also use this idea. For each node, Graphsage tries to aggregate the features of its neighbors to get structual information. And then they combine neighbors' information and its own information to generate new information. This new information is the embedding for the node, containing structual information(neighbors' features) and own features.\n",
        "\n",
        "First, they **sample a fixed number** of unique neighbors for each node(to speed up, and experiment shows it won't influence the performance). \n",
        "\n",
        "Second, they use aggregator to aggregate the features of these neighbors for each node. The authors proposed three types of aggregators, **mean, max-pooling, and LSTM**. The core idea of designing aggregator is to make sure it's **symmetric** while training because it has to operate over an unordered set of vectors.\n",
        "\n",
        "Third, we get fused neighbors' features for each node. We use this fused feature to **combine** with the feature of center node and **project** this combined feature to a **lower** space to reduce the dimension of each node.\n",
        "\n",
        "Fourth, iterate step 1 to 3 many times and use loss function to update weights of aggregators and projectors, we could get final model. We could train this model as unsupervised or supervised model. Below is the example of supervised model."
      ],
      "metadata": {
        "id": "MDtmiwSBh3Ys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOb8aCd3K5p0",
        "outputId": "fb31b66f-dbe9-4ead-b38b-a640f3fb4731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 705 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 4.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=6fc326357521f937a0925818e7d447e6f1a983970769009c423cb6ece8c06ae0\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import Planetoid\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "7Eo4wBQdQrgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
        "nodenum = dataset.data.num_nodes\n",
        "edges = dataset.data.edge_index.T\n",
        "nodes = defaultdict(lambda: defaultdict(int))\n",
        "for edge in edges:\n",
        "  nodes[edge[0].item()][edge[1].item()] += 1\n",
        "  nodes[edge[1].item()][edge[0].item()] += 1"
      ],
      "metadata": {
        "id": "8O14vVYgQv7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38da5c30-3901-41e3-ff46-39d90d89452e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = dataset[0].to(device)\n",
        "#features = data.x\n",
        "features = {}\n",
        "for i in range(data.x.shape[0]):\n",
        "  features[i] = data.x[i].unsqueeze(0)"
      ],
      "metadata": {
        "id": "4lfVENmTRyGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class alias():\n",
        "  def __init__(self, probs):\n",
        "    self.n = len(probs)\n",
        "    self.scaledprobs = {}\n",
        "    self.table = {}\n",
        "    self.aliastable = {}\n",
        "    self.small = []\n",
        "    self.big = []\n",
        "    self.keys = list(probs.keys())\n",
        "\n",
        "    for item in probs:\n",
        "      prob = probs[item]\n",
        "      self.scaledprobs[item] = prob * self.n\n",
        "      if self.scaledprobs[item] > 1:\n",
        "        self.big.append(item)\n",
        "      elif self.scaledprobs[item] < 1:\n",
        "        self.small.append(item)\n",
        "      else:\n",
        "        self.table[item] = 1\n",
        "    \n",
        "    while self.small and self.big:\n",
        "      smallitem = self.small.pop()\n",
        "      bigitem = self.big.pop()\n",
        "      newprob = self.scaledprobs[bigitem] - (1 - self.scaledprobs[smallitem])\n",
        "      self.table[smallitem] = self.scaledprobs[smallitem]\n",
        "      self.aliastable[smallitem] = bigitem\n",
        "      self.scaledprobs[bigitem] = newprob\n",
        "      if self.scaledprobs[bigitem] > 1:\n",
        "        self.big.append(bigitem)\n",
        "      elif self.scaledprobs[bigitem] < 1:\n",
        "        self.small.append(bigitem)\n",
        "      else:\n",
        "        self.table[bigitem] = 1\n",
        "    \n",
        "    while self.small:\n",
        "      smallitem = self.small.pop()\n",
        "      self.table[smallitem] = 1\n",
        "    \n",
        "    while self.big:\n",
        "      bigitem = self.big.pop()\n",
        "      self.table[bigitem] = 1\n",
        "\n",
        "  def sampling_one(self):\n",
        "    sample = random.choice(self.keys)\n",
        "    if self.table[sample] >= random.uniform(0, 1):\n",
        "      return sample\n",
        "    else:\n",
        "      return self.aliastable[sample]\n",
        "  \n",
        "  def sampling_n(self, n):\n",
        "    samples = []\n",
        "    for i in range(n):\n",
        "      samples.append(self.sampling_one())\n",
        "    return samples"
      ],
      "metadata": {
        "id": "-a_PKnDWUU8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neighbors = {}\n",
        "for node in nodes:\n",
        "  neighbors[node] = alias(nodes[node])"
      ],
      "metadata": {
        "id": "7OJ5ulpvVHc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batchsz = 64"
      ],
      "metadata": {
        "id": "y1Vz7j22iGsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = torch.IntTensor(nodenum, 1)\n",
        "for i in range(nodenum):\n",
        "  xs[i] = i"
      ],
      "metadata": {
        "id": "iMM6kgUjjUIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torch.utils.data.TensorDataset(xs[data.train_mask].to(device), data.y[data.train_mask])\n",
        "train_loader = DataLoader(trainset, batch_size=batchsz, shuffle=True)"
      ],
      "metadata": {
        "id": "XHtAcTath26L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Graphsage(nn.Module):\n",
        "  def __init__(self, neighbors, features, k, n, dims):\n",
        "    super(Graphsage, self).__init__()\n",
        "    self.neighbors = neighbors\n",
        "    self.features = features\n",
        "    self.k = k\n",
        "    self.n = n\n",
        "    self.aggregator = nn.ModuleList([nn.Linear(dims[i], dims[i], bias=True) for i in range(self.k)])\n",
        "    self.linears = nn.ModuleList([nn.Linear(2 * dims[i - 1], dims[i], bias=False) for i in range(1, self.k + 1)])\n",
        "    self.mlp = nn.Linear(dims[-2], dims[-1])\n",
        "    self.bns = nn.ModuleList([nn.BatchNorm1d(dims[i]) for i in range(1, self.k + 1)])\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax()\n",
        "\n",
        "  def forward(self, batch):\n",
        "    batchs = defaultdict(set)\n",
        "    batchs[self.k] = set()\n",
        "    batchneighbors = defaultdict(lambda: defaultdict(list))\n",
        "    batchlist = []\n",
        "    \n",
        "    for node in batch:\n",
        "      batchs[self.k].add(node.item())\n",
        "      batchlist.append(node.item())\n",
        "\n",
        "    for i in range(self.k, 0, -1):\n",
        "      batchs[i - 1] |= batchs[i]\n",
        "      for node in batchs[i]:\n",
        "        kneighbors = set(self.neighbors[node].sampling_n(self.n))\n",
        "        batchs[i - 1] |= kneighbors\n",
        "        batchneighbors[i - 1][node] = list(kneighbors)\n",
        "\n",
        "    fs = deepcopy(self.features)\n",
        "\n",
        "    for i in range(1, self.k + 1):\n",
        "      aggregate = {}\n",
        "      for node in batchs[i]:\n",
        "        for neigh in batchneighbors[i - 1][node]:\n",
        "          if node not in aggregate:\n",
        "            aggregate[node] = fs[neigh]\n",
        "          else:\n",
        "            aggregate[node] = torch.cat((aggregate[node], fs[neigh]), dim=0)\n",
        "        aggregate[node] = torch.mean(aggregate[node], dim=0, keepdim=True)\n",
        "        aggregate[node] = self.aggregator[i - 1](aggregate[node])\n",
        "        aggregate[node] = torch.cat((aggregate[node], fs[node]), dim=1)\n",
        "        aggregate[node] = self.linears[i - 1](aggregate[node])\n",
        "        aggregate[node] = self.relu(aggregate[node])\n",
        "      \n",
        "      tmp = list(batchs[i])\n",
        "      agg = aggregate[tmp[0]]\n",
        "      for j in range(1, len(tmp)):\n",
        "        agg = torch.cat((agg, aggregate[tmp[j]]), dim=0)\n",
        "\n",
        "      agg = self.bns[i - 1](agg)\n",
        "\n",
        "      for j, node in enumerate(tmp):\n",
        "        fs[node] = agg[j].unsqueeze(0)\n",
        "\n",
        "    z = fs[batchlist[0]]\n",
        "    for i in range(1, len(batchlist)):\n",
        "      z = torch.cat((z, fs[batchlist[i]]))\n",
        "    \n",
        "    result = self.softmax(z)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "kaVeODEaVbyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "epochs = 50\n",
        "k = 3\n",
        "n = 20\n",
        "dims = [dataset.num_node_features]\n",
        "diff = (dataset.num_node_features - dataset.num_classes) // k\n",
        "for i in range(k):\n",
        "  dims.append(dims[0] - diff * (i + 1))\n",
        "\n",
        "dims.append(dataset.num_classes)"
      ],
      "metadata": {
        "id": "uStSHRIQg1Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Graphsage(neighbors, features, k, n, dims).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.NLLLoss()"
      ],
      "metadata": {
        "id": "AV90KMEHg7aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "  acc = 0\n",
        "  for x, y in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    out = model(x)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, pred = out.max(dim=1)\n",
        "    acc += float(pred.eq(y).sum().item())\n",
        "  print(\"epoch: {0}, loss: {1}, train acc: {2}\".format(epoch, loss.item(), acc / data.train_mask.sum().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8AGgj4ThZ17",
        "outputId": "68c52796-a59a-4eb4-be34-6c3e3d96d2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, loss: 2.1925950050354004, train acc: 0.1357142857142857\n",
            "epoch: 1, loss: 1.367342472076416, train acc: 0.4714285714285714\n",
            "epoch: 2, loss: 1.1162117719650269, train acc: 0.6714285714285714\n",
            "epoch: 3, loss: 1.3178423643112183, train acc: 0.6928571428571428\n",
            "epoch: 4, loss: 1.7026225328445435, train acc: 0.7214285714285714\n",
            "epoch: 5, loss: 1.029147982597351, train acc: 0.8285714285714286\n",
            "epoch: 6, loss: 1.3029099702835083, train acc: 0.8214285714285714\n",
            "epoch: 7, loss: 0.6563180088996887, train acc: 0.9\n",
            "epoch: 8, loss: 1.2874561548233032, train acc: 0.8928571428571429\n",
            "epoch: 9, loss: 0.6392794251441956, train acc: 0.8928571428571429\n",
            "epoch: 10, loss: 0.49636831879615784, train acc: 0.9428571428571428\n",
            "epoch: 11, loss: 0.3361985683441162, train acc: 0.9571428571428572\n",
            "epoch: 12, loss: 0.2691052258014679, train acc: 0.9571428571428572\n",
            "epoch: 13, loss: 0.23887528479099274, train acc: 0.9571428571428572\n",
            "epoch: 14, loss: 0.5103762745857239, train acc: 0.9571428571428572\n",
            "epoch: 15, loss: 0.3982427418231964, train acc: 0.9714285714285714\n",
            "epoch: 16, loss: 0.1278974860906601, train acc: 0.9785714285714285\n",
            "epoch: 17, loss: 0.6832179427146912, train acc: 0.9714285714285714\n",
            "epoch: 18, loss: 0.07249858975410461, train acc: 0.9857142857142858\n",
            "epoch: 19, loss: 1.5514445304870605, train acc: 0.95\n",
            "epoch: 20, loss: 1.214835524559021, train acc: 0.9642857142857143\n",
            "epoch: 21, loss: 0.07225456833839417, train acc: 0.9928571428571429\n",
            "epoch: 22, loss: 0.318711519241333, train acc: 0.9714285714285714\n",
            "epoch: 23, loss: 0.8941466808319092, train acc: 0.9571428571428572\n",
            "epoch: 24, loss: 0.2706097662448883, train acc: 0.9857142857142858\n",
            "epoch: 25, loss: 0.07185671478509903, train acc: 0.9785714285714285\n",
            "epoch: 26, loss: 0.13985861837863922, train acc: 0.9785714285714285\n",
            "epoch: 27, loss: 0.4424569606781006, train acc: 0.9857142857142858\n",
            "epoch: 28, loss: 0.14413800835609436, train acc: 0.9857142857142858\n",
            "epoch: 29, loss: 0.37322676181793213, train acc: 0.9785714285714285\n",
            "epoch: 30, loss: 0.0773535817861557, train acc: 0.9928571428571429\n",
            "epoch: 31, loss: 0.7646409869194031, train acc: 0.9714285714285714\n",
            "epoch: 32, loss: 0.4078131914138794, train acc: 0.9857142857142858\n",
            "epoch: 33, loss: 0.29842808842658997, train acc: 0.9785714285714285\n",
            "epoch: 34, loss: 0.028492696583271027, train acc: 0.9928571428571429\n",
            "epoch: 35, loss: 0.48163697123527527, train acc: 0.9857142857142858\n",
            "epoch: 36, loss: 0.0491931177675724, train acc: 0.9857142857142858\n",
            "epoch: 37, loss: 0.2554291784763336, train acc: 0.9714285714285714\n",
            "epoch: 38, loss: 0.0647197887301445, train acc: 0.9642857142857143\n",
            "epoch: 39, loss: 0.7603091597557068, train acc: 0.9357142857142857\n",
            "epoch: 40, loss: 0.8400330543518066, train acc: 0.95\n",
            "epoch: 41, loss: 0.4755135774612427, train acc: 0.9642857142857143\n",
            "epoch: 42, loss: 0.6980590224266052, train acc: 0.9642857142857143\n",
            "epoch: 43, loss: 0.5763885378837585, train acc: 0.9714285714285714\n",
            "epoch: 44, loss: 0.39894601702690125, train acc: 0.9642857142857143\n",
            "epoch: 45, loss: 0.04836682602763176, train acc: 0.9928571428571429\n",
            "epoch: 46, loss: 0.0625510960817337, train acc: 0.9857142857142858\n",
            "epoch: 47, loss: 0.2688930332660675, train acc: 0.9785714285714285\n",
            "epoch: 48, loss: 0.3464023768901825, train acc: 0.9642857142857143\n",
            "epoch: 49, loss: 0.2816873788833618, train acc: 0.9857142857142858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torch.utils.data.TensorDataset(xs[data.test_mask].to(device), data.y[data.test_mask])\n",
        "test_loader = DataLoader(testset, batch_size=batchsz, shuffle=True)"
      ],
      "metadata": {
        "id": "4s_Eo91hoVvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "acc = 0\n",
        "for x, y in test_loader:\n",
        "  out = model(x)\n",
        "  _, pred = out.max(dim=1)\n",
        "  acc += float(pred.eq(y).sum().item())\n",
        "print(\"test acc: {0}\".format(acc / data.test_mask.sum().item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkGo7gjzoWEw",
        "outputId": "82a563d9-d65d-45f0-9083-5ac39ee3b403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test acc: 0.747\n"
          ]
        }
      ]
    }
  ]
}